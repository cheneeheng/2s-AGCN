aagcn:
        - 10 layers:
                  - original, old dataset generation:
                            - 94.7%
                            - overfit, train acc ~1
                  - original, 4gpu:
                            - 94.5%
                            - overfit, train acc ~1
                  - original:
                            - 94.3%
                            - overfit, train acc ~1
                  - no attention:
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
                  - no pad:
                            - 94.3%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.3%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.8-1.2):
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.5-1.5):
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
        - 6 layers:
                  - original:
                            - 94.4%
                            - overfit, train acc ~1
                            - almost identical to the 10 layers.
                  - no attention:
                            - 93.8%
                            - overfit, train acc ~1
                  - no pad:
                            - 94.1%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
        - 3 layers:
                  - original:
                            - 93.2%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no attention:
                            - 92.6%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad:
                            - 93.1%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad, no attention:
                            - 92.5%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1

aagcn_v2:
        - A does not depend on predefined A matrix. > no noticable difference.
        - Using 1 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1
        - Using 3 subset:
                  - no noticable difference. 94.1%
                  - overfit, train acc ~1
        - Using 5 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1

aagcn_v3:
        - Added additional projection in the GCN.
        - no noticable changes. 94.6%
        - overfit, train acc ~1

aagcn_v4:
        - Merged TCN into GCN.
        - Create different sets of subsets.
        - Each set will have different kernel size in temporal dim. (1,3,6,9)
        - did not converge

aagcn_v5:
        - added se block after TCN, sct attention is removed:
                  - slightly worse 93.72%
                  - overfit, train acc ~1
        - without the tse block, sct attention is removed.

aagcn_v6:
        - removed TCN. Added tcn conv in AdaptiveGCN (conv_d)
        - slightly worst 93%
        - overfit, train acc ~1

aagcn_v7:
        - removed TCN, added TSE in the AdaptiveGCN.
        - worst, 92.1%

aagcn_v8:
        - creates multiple attentions instead of one in AdaptiveGCN
        - using 5 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1
        - using 3 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1

aagcn_v9:
        - uses lstm before classification.
        - GAP-TV, proj4:
                  - 93.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 6layers:
                  - 92.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 3layers:
                  - 91.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, bi:
                  - 93.7%
                  - overfit, faster than original model, train acc ~1
                  - not much difference compared to lstm

aagcn_v10:
        - uses MHA before classification.
        - GAP-TV, 1h:
                  - 92%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h:
                  - 92.4%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 6layers:
                  - 91.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 3layers:
                  - 91.5%
                  - overfit, faster than original model, train acc ~1
        - GAP-T, 1h:
                  - 91.2%
                  - overfit, slower than original model, train acc ~1
        - GAP-T, 2h:
                  - 91%
                  - overfit, slower than original model, train acc ~1
        - Flat, 2h:
                  - 84.2%
                  - No Overfitting

aagcn_v11:
        - uses transformer layers with projection
        - COMPARED TO aagcn_v13:
                  - feature projection with increasing dim and striding is causing the non convergence.
        - 211217153001_cls_pos_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_cls_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_gap_mt_vc_2h_1a:
                  - did not converge
        - 211218220001_pos_gap_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout_noproj_torchtrans:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_torchtrans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_lowerlr:
                  - 88.9%
                  - overfit, train acc ~1

aagcn_v12:
        - uses attention learned from conv. (unary)
        - 211216170001, with relu for attention:
                  - 90.0 %.
                  - did not overfit, may need longer training. !!!!!
        - 211216170001_tanh, with tanh for attention:
                  - 94.2%
                  - almost identical to the original.

aagcn_v13:
        - uses original transformer layers + 1 aagcn projection block.
        - 211219110001_pos_cls_3trans:
                  - ~70%
                  - converge but training was unstable.
        - 211219110001_pos_cls_3trans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm_64dim:
                  - 89.4%
                  - overfit, train acc ~1
        - 211219110001_cls_3trans_lowerlr_prenorm:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm:
                  - 89.7%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_5trans_lowerlr_prenorm:
                  - 89.9%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_8trans_lowerlr_prenorm:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm:
                  - 90.2%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm_nopad:
                  - 90.6%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_15trans_lowerlr_prenorm:
                  - 89.8%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.4%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220100001_3base_pos_cls_3trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm:
                  - 89%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_4h:
                  - 89.7%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_8h:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_1h:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220230001_0base_pos_cls_3trans_lowerlr_prenorm:
                  - 81.3%
                  - did not overfit.
        - 211220230001_0base_pos_cls_5trans_lowerlr_prenorm:
                  - 84%
                  - did not overfit.
        - 211220230001_0base_pos_cls_8trans_lowerlr_prenorm:
                  - 83.9%
                  - did not overfit.
        - 211220230001_0base_pos_cls_10trans_lowerlr_prenorm:
                  - 83.1%
                  - did not overfit.
        - 211221113001_pos_gap_3trans_lowerlr_prenorm:
                  - 88.8%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_5trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_8trans_lowerlr_prenorm:
                  - 90.0%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm:
                  - 89.5%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm_nopad:
                  - 90.8%
                  - overfit, train acc ~1
aagcn_v14:
        - uses original transformer to replace tcn
        - 211221123001_pos_1trans_3l_prenorm:
                  - 84.8%
                  - overfit, train acc ~1
aagcn_v16:
        - puts positional encoder into tcn
        - 211221140001:
                  - 93.9%
                  - overfit, train acc ~1
aagcn_v17:
        - change tcn to create patches. (no overlap during conv)
        - 211228140001_nopad:
                  - diverge
        - 211228140001_nopad_3ks:
                  - 87%
                  - almost overfit, train acc ~1
        - 211228140001_nopad_10ks:
                  - diverge
        - 211228140001_nopad_3ks_5trans:
                  - 80.4%
                  - almost overfit, train acc ~1
                  - only converged midway
        - 211228140001_nopad_3ks_adam:
                  - diverged
        - 211228140001_nopad_3ks_adam_0_01lr:
                  - diverged
        - 211228140001_nopad_3ks_64dim:
                  - diverged
        - 211228140001_nopad_5ks_64dim:
                  - diverged

        - 220111210001_nopad_3ks_cossin:
                  - diverged
        - 220111210001_nopad_3ks_cossin_lowerlr:
                  - 89.2%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_noaug_lowerlr:
                  - 90.7%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_100div_noaug:
                  - converged at warmup but diverged afterwards.
        - 220111210001_nopad_3ks_cossin_100div_noaug_lowerlr:
                  - 90.5%
                  - overfit, train acc ~1
                  - converged really fast at the beginning (val acc was higher)
        - 220111210001_nopad_3ks_cossin_100div_lowerlr:
                  - 87.5%
                  - overfit, train acc ~1

        - 220111210001_nopad_3ks_rerun:
                  - same as 211228140001_nopad_3ks
                  - 87%
                  - almost overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_lowerlr:
                  - 84.6%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_noaug:
                  - 77.8%
                  - only converged midway
        - 220111210001_nopad_3ks_rerun_noaug_lowerlr:
                  - 88.2%
                  - converged really fast at the beginning (val acc was higher)
        - 220112190001_nopad_3ks_noaug_lowerlr_dropout05:
                  - 86.2%
                  - almost overfit, train acc ~1

        - 220112190001_nopad_3ks_noaug_lowerlr_attnmask_softmask_testing:
                  - uses attention masking => doest seem to have a diff
                  - 89.0%
                  - almost overfit, train acc ~1
        - 220112190001_nopad_3ks_noaug_lowerlowerlr_attnmask_forward:
                  - uses forward masking.
                  - masking starts after warmup epochs.
                  - goes up during warmup epochs and decreases afterwards.
                  - 24.4%
                  - almost overfit, train acc ~1

        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu:
                  - no relu after tcngcn unit.
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin:
                  - no relu after tcngcn unit.
                  - 90.2%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin_mul:
                  - no relu after tcngcn unit.
                  - 88.6%
                  - almost overfit, train acc ~1

aagcn_v18:
        - added shift operation after the gcn. This forward concats the features.
        - 220103110001_nopad:
                  - diverged
        - 220103110001_nopad_5ks:
                  - 73.1%
                  - almost overfit, train acc ~1

aagcn_v19:
        - dual transformer in helix form. spatial + temporal, 2 PE.
        - 220112150001_nopad_3ks:
                  - diverged
        - 220112150001_nopad_3ks_cossin:
                  - diverged
        - 220112150001_nopad_3ks_lowerlr_noaug:
                  - 89.5%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA:
                  - 90.0%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA_96d:
                  - 89.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_PA:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr:
                  - 83.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA:
                  - 85.3%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug:
                  - 89.7%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - 89.9%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

        - attn_220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - spatial attention seems to be work as intended for some cases.
                  - it is always highly influenced by J1
                  - there are cases where it is attending "correctly" but false prediction
                  - i.e. 43-7, 60, 1

        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlowerlr_noaug:
                  - 85.4%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug_shorterlr:
                  - 89.3%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

aagcn_v20:
        - added dual transformer, and merge at the end.
        - 220114130001_nopad_3ks_lowerlr_noaug:
                  - 87.1%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin:
                  - 88.7%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin_64d:
                  - 89.5%
                  - almost overfit, train acc ~1

aagcn_v21:
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_Nonedp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_0dp:
                  - 88.1%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.4%
                  - almost overfit, train acc ~1

aagcn_v22:
        - adds pe to each of the layers. v17
        - 220117190001_nopad_3ks_noaug_lowerlr:
                  - 89.1%
                  - almost overfit, train acc ~1
        - 220117190001_nopad_3ks_noaug_lowerlr_cossin:
                  - 90.4%
                  - almost overfit, train acc ~1

aagcn_v23:
        - adds pe to each of the layers. v20
        - 220117210001_nopad_3ks_lowerlr_noaug:
                  - 87.7%
                  - almost overfit, train acc ~1
        - 220117210001_nopad_3ks_lowerlr_noaug_cossin:
                  - 89.1%
                  - almost overfit, train acc ~1
