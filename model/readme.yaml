aagcn:
        - xsub:
                  - ./data/data/openpose_b25_j15_ntu_result/xview/aagcn_joint/220314090001
                  - 85.6%
        - xview:
                  - ./data/data/openpose_b25_j15_ntu_result/xview/aagcn_joint/220314100001
                  - 93.0%
                  - ./data/data/openpose_b25_j15_ntu_result/xview/aagcn_joint/220314100001_lowerlr_1em2lr
                  - 92.6%
        - ddp:
                  - 211209190001:
                            - 94.3%
                  - 220315183001_ddp:
                            - 91.5%
                  - 220315183001_ddp_syncbn:
                            - 91.7%
                  - 220315183001_ddp_syncbn_epochupdate_002lr:
                            - 93.9%
                  - 220315183001_ddp_syncbn_epochupdate_01lr:
                            - 94.6%
                  - 220315183001_ddp_syncbn_epochupdate_01lr_coslr:
                            - 94.4%
                  - 220323223001_ddp_nopad_norot:
                            - 92.6%
        - 10 layers:
                  - original, old dataset generation:
                            - 94.7%
                            - overfit, train acc ~1
                  - original, 4gpu:
                            - 94.5%
                            - overfit, train acc ~1
                  - original:
                            - 94.3%
                            - overfit, train acc ~1
                  - no attention:
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
                  - no pad:
                            - 94.3%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.3%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.8-1.2):
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.5-1.5):
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
        - 6 layers:
                  - original:
                            - 94.4%
                            - overfit, train acc ~1
                            - almost identical to the 10 layers.
                  - no attention:
                            - 93.8%
                            - overfit, train acc ~1
                  - no pad:
                            - 94.1%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
        - 3 layers:
                  - original:
                            - 93.2%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no attention:
                            - 92.6%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad:
                            - 93.1%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad, no attention:
                            - 92.5%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1

aagcn_v2:
        - A does not depend on predefined A matrix. > no noticable difference.
        - Using 1 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1
        - Using 3 subset:
                  - no noticable difference. 94.1%
                  - overfit, train acc ~1
        - Using 5 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1

aagcn_v3:
        - Added additional projection in the GCN.
        - no noticable changes. 94.6%
        - overfit, train acc ~1

aagcn_v4:
        - Merged TCN into GCN.
        - Create different sets of subsets.
        - Each set will have different kernel size in temporal dim. (1,3,6,9)
        - did not converge

aagcn_v5:
        - added se block after TCN, sct attention is removed:
                  - slightly worse 93.72%
                  - overfit, train acc ~1
        - without the tse block, sct attention is removed.

aagcn_v6:
        - removed TCN. Added tcn conv in AdaptiveGCN (conv_d)
        - slightly worst 93%
        - overfit, train acc ~1

aagcn_v7:
        - removed TCN, added TSE in the AdaptiveGCN.
        - worst, 92.1%

aagcn_v8:
        - creates multiple attentions instead of one in AdaptiveGCN
        - using 5 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1
        - using 3 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1

aagcn_v9:
        - uses lstm before classification.
        - GAP-TV, proj4:
                  - 93.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 6layers:
                  - 92.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 3layers:
                  - 91.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, bi:
                  - 93.7%
                  - overfit, faster than original model, train acc ~1
                  - not much difference compared to lstm

aagcn_v10:
        - uses MHA before classification.
        - GAP-TV, 1h:
                  - 92%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h:
                  - 92.4%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 6layers:
                  - 91.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 3layers:
                  - 91.5%
                  - overfit, faster than original model, train acc ~1
        - GAP-T, 1h:
                  - 91.2%
                  - overfit, slower than original model, train acc ~1
        - GAP-T, 2h:
                  - 91%
                  - overfit, slower than original model, train acc ~1
        - Flat, 2h:
                  - 84.2%
                  - No Overfitting

aagcn_v11:
        - uses transformer layers with projection
        - COMPARED TO aagcn_v13:
                  - feature projection with increasing dim and striding is causing the non convergence.
        - 211217153001_cls_pos_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_cls_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_gap_mt_vc_2h_1a:
                  - did not converge
        - 211218220001_pos_gap_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout_noproj_torchtrans:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_torchtrans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_lowerlr:
                  - 88.9%
                  - overfit, train acc ~1

aagcn_v12:
        - uses attention learned from conv. (unary)
        - 211216170001, with relu for attention:
                  - 90.0 %.
                  - did not overfit, may need longer training. !!!!!
        - 211216170001_tanh, with tanh for attention:
                  - 94.2%
                  - almost identical to the original.

aagcn_v13:
        - uses original transformer layers + 1 aagcn projection block.
        - 211219110001_pos_cls_3trans:
                  - ~70%
                  - converge but training was unstable.
        - 211219110001_pos_cls_3trans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm_64dim:
                  - 89.4%
                  - overfit, train acc ~1
        - 211219110001_cls_3trans_lowerlr_prenorm:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm:
                  - 89.7%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_5trans_lowerlr_prenorm:
                  - 89.9%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_8trans_lowerlr_prenorm:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm:
                  - 90.2%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm_nopad:
                  - 90.6%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_15trans_lowerlr_prenorm:
                  - 89.8%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.4%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220100001_3base_pos_cls_3trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm:
                  - 89%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_4h:
                  - 89.7%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_8h:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_1h:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220230001_0base_pos_cls_3trans_lowerlr_prenorm:
                  - 81.3%
                  - did not overfit.
        - 211220230001_0base_pos_cls_5trans_lowerlr_prenorm:
                  - 84%
                  - did not overfit.
        - 211220230001_0base_pos_cls_8trans_lowerlr_prenorm:
                  - 83.9%
                  - did not overfit.
        - 211220230001_0base_pos_cls_10trans_lowerlr_prenorm:
                  - 83.1%
                  - did not overfit.
        - 211221113001_pos_gap_3trans_lowerlr_prenorm:
                  - 88.8%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_5trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_8trans_lowerlr_prenorm:
                  - 90.0%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm:
                  - 89.5%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm_nopad:
                  - 90.8%
                  - overfit, train acc ~1
aagcn_v14:
        - uses original transformer to replace tcn
        - 211221123001_pos_1trans_3l_prenorm:
                  - 84.8%
                  - overfit, train acc ~1
aagcn_v16:
        - puts positional encoder into tcn
        - 211221140001:
                  - 93.9%
                  - overfit, train acc ~1
aagcn_v17:
        - change tcn to create patches. (no overlap during conv)
        - uses transformer - temporal
        - 211228140001_nopad:
                  - diverge
        - 211228140001_nopad_3ks:
                  - 87%
                  - almost overfit, train acc ~1
        - 211228140001_nopad_10ks:
                  - diverge
        - 211228140001_nopad_3ks_5trans:
                  - 80.4%
                  - almost overfit, train acc ~1
                  - only converged midway
        - 211228140001_nopad_3ks_adam:
                  - diverged
        - 211228140001_nopad_3ks_adam_0_01lr:
                  - diverged
        - 211228140001_nopad_3ks_64dim:
                  - diverged
        - 211228140001_nopad_5ks_64dim:
                  - diverged
        - 220209100001_nopad_3ks_noaug_lowerlr_nope:
                  - 88.3%
        # COSSIN PE -----------------------------------------
        - 220111210001_nopad_3ks_cossin:
                  - diverged
        - 220111210001_nopad_3ks_cossin_lowerlr:
                  - 89.2%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_noaug_lowerlr:
                  - 90.7%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_100div_noaug:
                  - converged at warmup but diverged afterwards.
        - 220111210001_nopad_3ks_cossin_100div_lowerlr:
                  - 87.5%
                  - overfit, train acc ~1
        - 220125120001_nopad_3ks_lowerlr_100div_cossin_rerun_randommoveshift:
                  - 89.5%
                  - almost overfit, train acc ~1
        - ...: # 220125120001_nopad_3ks_lowerlr_100div_cossin_rerun_randommoveshift_drop_out:
                  - 89.5%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_8d_25h:
                  - 90.0%
        - 220111210001_nopad_3ks_cossin_100div_noaug_lowerlr: #####
                  - 90.5%
                  - overfit, train acc ~1
                  - converged really fast at the beginning (val acc was higher)
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_25h:
                  - 91.2%
        - 220209100001_nopad_3ks_noaug_lowerlr_100div_cossin_25h_6l:
                  - 91.0%
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_32d:
                  - 90.4%
                  - almost overfit, train acc ~1
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_64d:
                  - 90.5%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_64d_25h:
                  - 91.2%
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_128d:
                  - 89.7%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_128d:
                  - 89.7%
        - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_4h:
                  - did not converge.
        - 220201210001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_8h:
                  - 91.3%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_16h:
                  - 91.5%
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_32d: (0.5dropout)
                  - 88.9%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_64d: (0.5dropout)
                  - 85.0%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_128d: (0.5dropout)
                  - 87.6%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_256d: (0.5dropout)
                  - GPU mem error
        # RERUN WITH POS PE -----------------------------------------
        - 220111210001_nopad_3ks_rerun:
                  - same as 211228140001_nopad_3ks
                  - 87%
                  - almost overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_lowerlr:
                  - 84.6%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_noaug:
                  - 77.8%
                  - only converged midway
        - 220111210001_nopad_3ks_rerun_noaug_lowerlr:
                  - 88.2%
                  - converged really fast at the beginning (val acc was higher)
        # ATTENTION MASKING -----------------------------------------
        - 220112190001_nopad_3ks_noaug_lowerlr_attnmask_softmask_testing:
                  - uses input frame mask
                  - uses attention masking => doest seem to have a diff
                  - 89.0%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
                  - only have access to past info.
                  - 89.2%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
                  - only have access to future info.
                  - 90.5%

        # - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
        #           - buggy, wrong logic
        #           - 89.3%
        # - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
        #           - buggy, wrong logic
        #           - 90.5%
        # - 220112190001_nopad_3ks_noaug_lowerlowerlr_attnmask_forward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - uses forward masking.
        #           - masking starts after warmup epochs.
        #           - goes up during warmup epochs and decreases afterwards.
        #           - 24.4%
        # - 220125120001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - uses forward masking in last layer.
        #           - 89.1%
        #           - almost overfit, train acc ~1
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - backward masking
        #           - 90.5%
        #           - almost overfit, train acc ~1
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_backward_128d_8h:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - no backward
        #           - 91.3%
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_8h_backward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - 91.3%
        # - 220201210001_..._128d_8h_backward_rerun:
        #           - - BUGGY MASKING NOT APPLIED AT EVAL
        #           - 91.3%

        # NO RELU IN TCNGCN UNIT -----------------------------------------
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu:
                  - no relu after tcngcn unit.
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin:
                  - no relu after tcngcn unit.
                  - 90.2%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin_mul:
                  - no relu after tcngcn unit.
                  - 88.6%
                  - almost overfit, train acc ~1
        # SAM ---------------------------------------------------------------
        - 220125120001_nopad_3ks_noaug_lowerlr_100div_cossin_sam:
                  - 90.5%
                  - almost overfit, train acc ~1
        # ADAM ---------------------------------------------------------------
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_adam:
                  - 52.9%
        # LN for data_norm -------------------------------------------------
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_ln:
                  - 89.2%
                  - almost overfit, train acc ~1
        # DROPOUT ----------------------------------------------------
        - 220112190001_nopad_3ks_noaug_lowerlr_dropout05:
                  - 86.2%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_nodropout:
                  - 89.1%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout01:
                  - 89.9%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout03:
                  - 90.4%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout04:
                  - 89.8%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout05:
                  - 89.0%
                  - almost overfit, train acc ~1
        # PROJ -------------------------------------
        - 220121110001_nopad_1ks_noaug_lowerlr_100div_cossin:
                  - 90.2%
                  - almost overfit, train acc ~1
        - 220121110001_nopad_2ks_noaug_lowerlr_100div_cossin:
                  - 90.6%
                  - almost overfit, train acc ~1

        # more gpu to reduce the batch size per gpu -----------------------
        - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_4gpu:
                  - 90.2%
                  - almost overfit, train acc ~1

        - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_2h:
                  - 90.4%
                  - almost overfit, train acc ~1

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_onecycliclr:
                  - 88.6%
                  - with backward
        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_cycliclr:
                  - 88.6%
                  - with backward

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_dropout05:
                  - 89.2%

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_nodecay:
                  - decay = 5e4 *******
                  - 88.9%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_higherwd:
                  - 1e-3 decay
                  - 88.9%

        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_16dffn:
                  - 89.6%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_256dffn:
                  - 91.0%

        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_stretch:
                  - remove zero padding and stretch the original values.
                  - 31.1%

aagcn_v18:
        - added shift operation after the gcn. This forward concats the features.
        - 220103110001_nopad:
                  - diverged
        - 220103110001_nopad_5ks:
                  - 73.1%
                  - almost overfit, train acc ~1

aagcn_v19:
        - dual transformer in helix form. spatial + temporal, 2 PE.
        - 220112150001_nopad_3ks:
                  - diverged
        - 220112150001_nopad_3ks_cossin:
                  - diverged
        - 220112150001_nopad_3ks_lowerlr_noaug:
                  - 89.5%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA:
                  - 90.0%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA_96d:
                  - 89.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_PA:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr:
                  - 83.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA:
                  - 85.3%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug:
                  - 89.7%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - 89.9%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

        - attn_220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - spatial attention seems to be work as intended for some cases.
                  - it is always highly influenced by J1
                  - there are cases where it is attending "correctly" but false prediction
                  - i.e. 43-7, 60, 1

        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlowerlr_noaug:
                  - 85.4%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug_shorterlr:
                  - 89.3%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

aagcn_v20:
        - added dual transformer, and merge at the end.
        - 220114130001_nopad_3ks_lowerlr_noaug:
                  - 87.1%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin:
                  - 88.7%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin_64d:
                  - 89.5%
                  - almost overfit, train acc ~1

aagcn_v21:
        - positional masking instead of PE at input.
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_Nonedp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_0dp:
                  - 88.1%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.4%
                  - almost overfit, train acc ~1

aagcn_v22:
        - adds pe to each of the layers. v17
        - 220117190001_nopad_3ks_noaug_lowerlr:
                  - 89.1%
                  - almost overfit, train acc ~1
        - 220117190001_nopad_3ks_noaug_lowerlr_cossin:
                  - 90.4%
                  - almost overfit, train acc ~1

aagcn_v23:
        - adds pe to each of the layers. v20
        - 220117210001_nopad_3ks_lowerlr_noaug:
                  - 87.7%
                  - almost overfit, train acc ~1
        - 220117210001_nopad_3ks_lowerlr_noaug_cossin:
                  - 89.1%
                  - almost overfit, train acc ~1

aagcn_v24:
        - only spatial transformer
        - 220118110001_nopad_3ks_noaug_lowerlr:
                  - 65.1%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_32d:
                  - 72.2%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_64d:
                  - 75.2%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_128d:
                  - 80.9%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_256d:
                  - 83.8%
                  - no overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin:
                  - 62.7%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_32d:
                  - 71.4%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_64_cossind:
                  - 75.6%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d: ####
                  - 84.3%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_256d:
                  - 83.7%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_5l:
                  - 84.1%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_7l:
                  - 83.1%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_4h:
                  - 82.2%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_8h:
                  - 82.6%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_16h:
                  - 83.9%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_2aagcn:
                  - 83.3%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_3aagcn:
                  - 80.3%
                  - almost overfit

        # - 220125170001_nopad_3ks_noaug_lowerlr_cossin_16d:
        #           - FAULTY, NO CLS TOKEN
        #           - cls masking
        #           - 60.9%
        # - 220125170001_nopad_3ks_noaug_lowerlr_cossin_128d:
        #           - FAULTY, NO CLS TOKEN
        #           - cls masking
        #           - 71.7%

        # - 220126100001_nopad_3ks_noaug_lowerlr_cossin_PAa:
        #           - FAULTY, NO CLS TOKEN
        #           - 59.9%
        #           - added PAa as attn mask.
        #           - with CLS_MASK
        # - 220126100001_nopad_3ks_noaug_lowerlr_cossin_3PAa:
        #           - FAULTY, NO CLS TOKEN
        #           - 66.9%
        #           - added 3 diff PAa as attn mask.
        #           - with CLS_MASK

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_CLS_MASK:
                  - with CLS_MASK
                  - 19.2%

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_PAa_144d:
                  - added 3 diff PAa as attn mask.
                  - 82.3%
                  - almost overfit
        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_PAa_144d_CLS_MASK:
                  - added 3 diff PAa as attn mask.
                  - with CLS_MASK
                  - 72.8%

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_3PAa_144d_CLS_MASK:
                  - added 3 diff PAa as attn mask.
                  - with CLS_MASK
                  - 73.3%
        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_3PAa_144d:
                  - added 3 diff PAa as attn mask.
                  - 82.3%
                  - almost overfit

aagcn_v25:
        - only temporal transformer with tcn proj
        - 220121103001_nopad_3ks_noaug_lowerlr:
                  - 89.5%
                  - almost overfit
        - 220121103001_nopad_3ks_noaug_lowerlr_cossin:
                  - 89.6%
                  - almost overfit

aagcn_v26:
        - only spatial transformer with tcn proj
        - 220121100001_nopad_3ks_noaug_lowerlr:
                  - 54.1%
        - 220121100001_nopad_3ks_noaug_lowerlr_cossin:
                  - 53.5%

aagcn_v27:
        - spatial transformer, deberta
        - 220124163001_nopad_3ks_noaug_lowerlr:
                  - 62.8%
        - 220124163001_nopad_3ks_noaug_lowerlre4:
                  - 2.73%
                  - did not converge
        - 220124163001_nopad_3ks_noaug_lowerlre5_adam:
                  - 6.23%
                  - did not converge

aagcn_v28:
        - temporal transformer, deberta
        - 220124183001_nopad_3ks_noaug_lowerlr:
                  - 89.7%
                  - almost overfit
        - 220124183001_nopad_3ks_noaug_lowerlre4:
                  - 73.4%
                  - almost overfit
        - 220124183001_nopad_3ks_noaug_lowerlre5_adam:
                  - 77.1%
                  - almost overfit
        - 220126153001_nopad_3ks_noaug_lowerlr_emd:
                  - 89.2%
                  - almost overfit
        - 220126153001_nopad_3ks_noaug_lowerlr_emd_128d:
                  - 90.0%
                  - almost overfit
        - 220207153001_nopad_3ks_noaug_lowerlr_emd_128d_8h:
                  - 91.1%
        - 220201213001_nopad_3ks_noaug_lowerlr_emd_span3:
                  - 88.8%
        - 220201213001_nopad_3ks_noaug_lowerlr_emd_span10:
                  - 89.2%

aagcn_v29:
        - from 20, merge FM after S and T transformer in each layer.
        - 220125200001_nopad_3ks_lowerlr_noaug_cossin:
                  - mean was applied to spatial trans before classifier
                  - 88.7%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin:
                  - concat features from both trans before classifier
                  - 87.9%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_16_128:
                  - concat features from both trans before classifier
                  - 88.0%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_PA:
                  - concat features from both trans before classifier
                  - PA for spatial trans
                  - 87.8%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_PAa:
                  - concat features from both trans before classifier
                  - PA for spatial trans wit gate variable.
                  - 88.0%
                  - almost overfit

aagcn_v30:
        - uses trans for tcn and gcn for spatial.
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin:
                  - 87.3%
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin_agcnv2:
                  - 85.3%
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin_64d:
                  - 89.4%

aagcn_v31:
        - based on aagcn_v19.
        - uses 1 PE.
        - sequential and alternates between temporal and spatial.
        - 220208220001_nopad_3ks_lowerlr_noaug_cossin:
                  - 89.1%
        - 220209120001_nopad_3ks_lowerlr_noaug_cossin_clstokensplit_newspatial:
                  - 89.8%
        - 220209120001_nopad_..._10sh:
                  - 90.1%
        - 220209120001_nopad_..._newspatial_streverse:
                  - 90.5%
        - 220209120001_nopad_3ks_lowerlr_noaug_cossin_sa-t:
                  - 91.0%
        - 220209120001_nopad_3ks_lowerlr_noaug_cossin_1l_sa-t:
                  - 89.9%
        - 220209120001_nopad_3ks_lowerlr_noaug_cossin_1l_sa-t-res_Aa:
                  - 90.2%
        - 220209120001_nopad_3ks_5e2lr_noaug_cossin_1l_sa-t:
                  - 89.6%
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_005lr:
                  - 89.5%
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr:
                  - 90.7%
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_10Aa_003lr:
                  - 90.7%
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr_rerun:
                  - debug correct.
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr_rerun2:
                  - either the code workflow or the LR adjust is causing it to fail.
                  - this uses the old LR adjust code, so workflow is most probably the cause.
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr_rerun3:
                  - debug run. training converging again.
                  - main_processor.py uses the old version, prior to refractor.
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr_rerun4:
                  - debug run. training converging again.
                  - main_processor.py moved utils after loaders.
        - 220216130001_nopad_3ks_noaug_cossin_1l_sa-t-res_Aa_003lr_rerun5:
                  - no converge after first epoch

aagcn_v32:
        - adding attention mask after softmax.
        - 220216190001_nopad_3ks_lowerlr_noaug_cossin_1l:
                  - 90.4%
        - 220216190001_nopad_3ks_lowerlr_noaug_cossin_1l_32d:
                  - 91.1%
        - 220216190001_nopad_3ks_lowerlr_noaug_cossin:
                  - 90.9%
        - adding alpha to attention and not the global mask.
        - 220217140001_nopad_3ks_lowerlr_noaug_cossin_1l_8sh:
                  - 90.5%
        - 220217140001_nopad_3ks_lowerlr_noaug_cossin_1l:
                  - 90.4%
        - 220217140001_nopad_3ks_lowerlr_noaug_cossin:
                  - 90.7%
        - 220218140001_nopad_3ks_lowerlr_noaug_cossin_nores_8d:
                  - 90.0%
        - 220218140001_nopad_3ks_lowerlr_noaug_cossin_nores:
                  - 91.1%
        - 220218140001_nopad_3ks_lowerlr_noaug_cossin_nores_pool:
                  - 91.1%

aagcn_v33:
        - added relative position encoding
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool:
                  - 91.1%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withA:
                  - 90.1%
        - 220221150001_nopad_3ks_lowerlr5e3_noaug_rel_pool_withAa_adamw:
                  - did not converge
        - 220221150001_nopad_3ks_lowerlr1e3_noaug_rel_pool_withAa_adamw:
                  - 88.0%
        - 220221150001_stretch_3ks_lowerlr_noaug_rel_pool_withAa:
                  - 90.0%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa:
                  - 91.1%
        - 220223110001_nopad_3ks_noaug_rel_pool_withAa_llrd:
                  - LR INVALID
                  - 88.3%
        - 220223110001_nopad_3ks_noaug_rel_pool_withAa_llrd_v3:
                  - LR INVALID
                  - 87.4%
        - 220224140001_nopad_3ks_noaug_rel_pool_withAa_v3_noPAfreeze_a1:
                  - LR INVALID
                  - 89.2%
        -  ########## TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_1h_v3_a1:
                  - 80.5%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_a1_GAPPOOL_v3:
                  - 82.2%
        - with m_mask. Binary masking to mask to temopral trans.
        - 220225150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_mmask_v3:
                  - 20.8%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_10h:
                  - 83.0%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_1h:
                  - 80.8%
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_16sh:
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_rerun:
        -  #  TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.##########
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_rerunori:
                  - debug run. training converging again.
                  - either the code workflow or the LR adjust is causing it to fail.
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_rerunori2:
                  - debug run. training converging again.
                  - either the code workflow or the LR adjust is causing it to fail.
                  - main_processor.py uses the old version, prior to refractor.
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_rerunori3:
                  - no converge
        - 220221150001_nopad_3ks_lowerlr_noaug_rel_pool_withAa_rerunori4:
                  - converdeg.
                  - adjust LR logic is wrong.

aagcn_v34:
        - from v33
        - uses different temporal transformer input format.
        -  ########## TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.
        - 220224160001_nopad_3ks_lowerlr_noaug_rel_v3:
                  - 78.6%
        - 220224160001_nopad_3ks_noaug_rel_v3_noPAfreeze_a1:
                  - 84.9%
        -  #  TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.##########

aagcn_v35:
        - clean copy from v33
        -  ########## TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.
        - 220228200001_nopad_3ks_noaug_rel_pool_withAa_1l_v1:
                  - LR INVALID
                  - 82.7%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAa_1l_16sh_v1:
                  - LR INVALID
                  - 83.1%
        -  #  TRAINING SEEMS TO NOT CONVERGE AS BEFFORE.##########
        - 220228200001_nopad_3ks_noaug_rel_pool_withAa_1l_v2:
                  - 90.3%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAa_1l_16sh_v2:
                  - 90.6%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_1l_v2:
                  - 90.5%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_1l_16sh_v2:
                  - 90.4%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_v2:
                  - 91.3%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_v2_llrd:
                  - 90.9%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_1l_paralleladd_v2:
                  - 87.9%
        - 220228200001_nopad_3ks_noaug_rel_pool_withAainv_v1:
                  - 91.0%
        - 220303130001_nopad_3ks_noaug_rel_pool_EmptyA_v2:
                  - 91.1%
        - 220228200001_nopad_3ks_noaug_rel_pool_EmptyA_scls_v1:
                  - 91.2%

aagcn_v36:
        - basee on v35
        - 2 streams and concat in the end.
        - 220304100001_nopad_3ks_noaug_rel_pool_EmptyA_1l_v1:
                  - 89.5%
        - 220304100001_nopad_3ks_noaug_rel_pool_EmptyA_3l_v1:
                  - 90.7%
        - 220304100001_nopad_3ks_noaug_rel_pool_EmptyA_5l_v1:
                  - 90.2%
        - 220304100001_nopad_3ks_noaug_rel_pool_EmptyA_cross_3l_v1:
                  - 90.2%
        - 220304100001_nopad_3ks_noaug_rel_pool_EmptyA_cross_5l_v1:
                  - 90.0%
        - 220307120001_nopad_3ks_noaug_rel_pool_EmptyA_cross_3l_coslr_v1:
                  - 90.5%
        # - 220307120001_nopad_3ks_noaug_rel_pool_EmptyA_cross_3l_coslrfrompaper_v1:
        #           - diverged with 0.004lr
        # - 220307120001_nopad_3ks_noaug_rel_pool_EmptyA_cross_3l_coslrfrompaper_v1:
        #           - diverged with 0.001lr
        # - 220307120001_nopad_3ks_noaug_rel_pool_EmptyA_cross_3l_coslrfrompaper_v1:
        #           - diverged with 0.0005lr, maybe because decay is 0.05
        #           - diverged during warmup
        # - 220307210001_nopad_3ks_noaug_rel_pool_3l_32f_coslr_v0:
        #           - diverged just after warmup
        # - 220307210001_nopad_3ks_noaug_rel_pool_3l_32f_noPAfreeze_coslrlong_v0:
        #           - cos with adam but very low LR
        #           - 90.0%
        # - 220307210001_nopad_3ks_noaug_rel_pool_3l_32f_noPAfreeze_coslrlongeps_v0:
        #           - cos with adamw (lr 1e-4, eps 1e-4)
        #           - 89.9%

aagcn_v37:
        - new transformer implementation
        - 220315153001_nopad_3ks_noaug_pool:
                  - 3s-t trans layers, 1 cross attention
                  - 89.0%
        - 220315153001_nopad_3ks_noaug_pool_cossin:
                  - 3s-t trans layers, 1 cross attention
                  - cossin PE
                  - 88.8%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp:
                  - 86.8%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn:
                  - ~87.5%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn_6l:
                  - 88.0%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn_0002lr:
                  - 90.3%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn_0002lr_6l:
                  - 90.8%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn_0002lr_6lSE:
                  - smaller dim head in attn module
                  - 90.8%
        - 220315153001_nopad_3ks_noaug_pool_cossin_ddp_syncbn_0002lr_6lSE2:
                  - smaller dim head in attn module + mlp
                  - 90.4%
        - 220315153001_pool_cossin_ddp_syncbn_0001lr_6lSE2_clip:
                  - ~70%
        - 220315153001_pool_cossin_ddp_syncbn_0001lr_6lSE2_adamw_clip:
                  - diverged
        - 220315153001_pool_cossin_ddp_syncbn_00002lr_6lSE2_clip:
                  - AdamW
                  - diverged after 10 epoch
        - 220315153001_pool_cossin_ddp_syncbn_00002lr_6lSE2_clip_coslr:
                  - AdamW
                  - diverged after 14 epochs
        - 220315153001_pool_cossin_ddp_syncbn_00002lr_9lSE2_clip_coslr:
                  - AdamW
                  - diverged after 7 epochs
        - 220315153001_pool_cossin_ddp_syncbn_00002lr_9l1cSE2_clip_coslr:
                  - AdamW
                  - diverged after 4 epochs

        # nopad, 3ks, noaug, pool, syncbn
        - 220319103001_cossin_ddp_9l1cSE2_8gpu:
                  - 6 heads, shrink s by 4, no shrink t
                  - 69.2%
        - 220319103001_cossin_ddp_9l1cSE2:
                  - 79.8%
        - 220319103001_cossin_ddp_9l3cSE2:
                  - 80.1%
        - 220319103001_cossin_ddp_9l1cSE2_aug:
                  - 61.2%
        - 220319103001_cossin_ddp_3l1cSE2_0bb:
                  - 76.0%
                  - base_lr = 0.002
        - 220319103001_cossin_ddp_3l1cSE2_0bb_002lr:
                  - 86.6%
        - 220319103001_cossin_ddp_3l1cSE2_0bb_01lr:
                  - 88.5%
        - 220319103001_cossin_ddp_9l3cSE2_0bb_01lr:
                  - 89.2%
        - 220319103001_cossin_ddp_9l1cSE2_0bb_01lr:
                  - 89.5%
        - 220319103001_cossin_ddp_9l1cSE2_0bb_01lr_coslr:
                  - 89.6%
        - 220319103001_cossin_ddp_9l1cSE2_0bb_001lr_coslr:
                  - 85.9%
        - 220319103001_cossin_ddp_9l1cSE3_0bb_01lr:
                  - 8 heads, no dim shrink for s, but expand for t
                  - 89.7%
        - 220319103001_cossin_ddp_18l2cSE4_0bb_01lr:
                  - 8 heads, no dim shrink (dim head for t is 25)
                  - 86.8%
        - 220319103001_cossin_ddp_9l1cSE4_0bb_01lr:
                  - 89.9%
        - 220319103001_cossin_ddp_9l1cSE4_0bb_01lr_0drop:
                  - 90.2%
        - 220319103001_cossin_ddp_9l1cSE4_0bb_01lr_01drop:
                  - 90.0%
        - 220319103001_cossin_ddp_9l1cSE5_0bb_01lr:
                  - 8 heads, no dim shrink, 4 depth for t
                  - 88.9%
        - 220319103001_cossin_ddp_9l1cSE6_0bb_01lr_subsample20:
                  - 6 heads, no dim change
                  - 83.7%
        - 220319103001_cossin_ddp_9l1cSE6_0bb_01lr_subsample100:
                  - 89.0%
        - 220319103001_cossin_ddp_9l1cSE6_0bb_01lr_subsample60:
                  - 88.3%
        - 220319103001_cossin_ddp_9l1cSE6_0bb_01lr_subsample60_randomrot:
                  - 86.8%

sgn:
        - 220319103001_ddp:
                  - 90.4%
        - 220319103001_ddp_30epoch_adam_128bs:
                  - 87.8%
        - 220319103001_ddp_120epoch:
                  - 91.8%
        - 220319103001_ddp_120epoch_adam:
                  - 92.0%
        - 220319103001_ddp_120epoch_adam_64bs:
                  - 91.9%
        - 220319103001_ddp_120epoch_adam_64bs_5test:
                  - 92.9%
        - 220319103001_ddp_120epoch_adam_64bs_nopad_norot:
                  - 91.8%
        - 220319103001_ddp_120epoch_adam_64bs_nopad_norot_5test:
                  - 92.8%
        - 220319103001_ddp_120epoch_adam_64bs_nopad_norot_centerff_5test:
                  - 93.7%
        - 220324163001_120epoch_adam_64bs_nopad_norot_centerff_5test:
                  - 92.8%
        - 220324163001_ori:
                  - with data processed from original repo
                  - 93.2% (best acc)
        - 220324163001_ori_1337:
                  - 93.3% (best acc)
        - 220324163001_ori_1337_rerun:
                  - 93.3% (best acc)
        - 220324163001_ori_1337_rerun_noworkerinit:
                  - 92.9% (best acc)
        - 220324163001_ori_1337_rerun_noworkerinit_pinmem:
                  - RUNS BEFORE HAS NUMPY NORMAL DISTRIBUTION
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_pinmem_rerun1:
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_pinmem_rerun2:
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_pinmem_rerun3:
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_rerun1:
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_rerun2:
                  - 94.2% (best acc)
        - 220327213001_1337_noworkerinit_rerun3:
                  - 94.2% (best acc)
        - 220327213001_1337:
                  - 93.8% (best acc)

sgn_v2:
        - 220327213001_1337:
                  - 94.4% (best acc)
        - 220329160001_part:
                  - shared_proj
                  - 94.0%
        - 220329160001_part_motion:
                  - shared_proj
                  - 94.3%
        - 220329160001_part_motion_aspp:
                  - shared_proj
                  - stopped halfway, very slow
                  - 93.8% at epoch ~70
        - 220329160001_part_motion_asppnodeterminitic:
                  - shared_proj
                  - 93.7%
        - 220329160001_part_motion_asppnodeterminitic_0135:
                  - shared_proj
                  - 94.1%
        - 220329160001_part_motion_asppnodeterminitic_0135_rerun:
                  - shared_proj
                  - 94.0%
        - 220331160001_subject:
                  - 94.1%
        - 220331160001_part:
                  - 94.3%
        - 220331160001_part_motion:
                  - 93.9%
        - 220331160001_part_subject:
                  - 93.9%
        - 220331160001_part_subject_aspp:
                  - 94.1%
        - 220331160001_part_subject_aspp_rerun:
                  - 94.3%
        - 220331160001_t3:
                  - 94.1%
        - 220331160001_t5:
                  - 94.4%
        - 220331160001_t7:
                  - 94.5%
        - 220331160001_t7_rerun:
                  - 94.47%
        - 220331160001_t9:
                  - 94.21%

        - 220401130001_nw32_t3:
                  - 94.08%
        - 220401130001_nw32_t5:
                  - 94.12%
        - 220401130001_nw32_t7:
                  - 94.13%
        - 220401130001_nw32_t9:
                  - 94.03%
        - 220401130001_nw32_subject:
                  - 94.22%
        - 220401130001_nw32_part1:
                  - 93.99%
        - 220401130001_nw32_part1_motion1:
                  - 93.69%
        - 220401130001_nw32_part1_motion2:
                  - 94.16%
        - 220401130001_nw32_tmaxpool3:
                  - 93.34%
        - 220401130001_nw32_tmaxpool5:
                  - 93.06%
        - 220401130001_nw32_tmaxpool7:
                  - 92.99%
        - 220401130001_nw32_tmaxpool9:
                  - 93.28%

        # joints 15
        - 220331160001_t9_j15:
                  - 90.55%
        - 220404110001:
                  - 90.72%

sgn_v3:
        - 220328170001_t3_gpshared:
                  - 92.9%
        - 220328170001_gpshared:
                  - 92.3%
        - 220328170001_t3:
                  - 93.2%
        - 220328170001_t3_x2dim:
                  - 93.3%
        - 220328170001_t3_x3dim:
                  - 93.3%
        - 220328170001_t3_x4dim:
                  - 93.3%
        - 220328170001_t3_gpshared_dropout:
                  - 92.9%

sgn_v4:
        - 220404120001:
                  - 94.09%
        - 220404120001_subject:
                  - 93.99%
        - 220404120001_part1_motion2:
                  - 94.10%
        # NO INIT IN WORKER
        - 220404120001_noinit_nw16:
                  - 94.13%
        - 220404120001_noinit_nw16_rerun:
                  - 94.13%
        - 220404120001_rerun_noinit:
                  - 94.48%
        - 220404120001_rerun2_noinit:
                  - 94.48%
        - 220404180001_jt2:
                  - 93.34%
        - 220404180001_jt3:
                  - 94.16%
        - 220404180001_fi2:
                  - 94.39%
        - 220404180001_fi3:
                  - 94.13%
        - 220404180001_subject:
                  - 94.14%
        - 220404180001_subject2:
                  - 94.18%
        - 220404180001_subject3:
                  - 94.45%
        - 220404180001_motionsampler:
                  - 94.86%
        - 220404180001_motionsampler_fullarrayshift:
                  - 94.86%
        - 220404180001_motionsampler_rerun:
                  - 94.86%
        - 220404180001_motionsampler_trainonly:
                  - 92.37%
        - 220404180001_motionsampler_valonly:
                  - 94.48%
        - 220404180001_part1_pt1:
                  - 93.89%
        - 220404180001_part1_pt1_motion2:
                  - 94.05%
        - 220404180001_part1_pt1_motion3:
                  - 94.36%
        - 220404180001_part1_pt1_motion4:
                  - 94.28%
        - 220404180001_part2:
                  - mode = 3, pt = 2
                  - 93.49%
        - 220404180001_part3_pt1:
                  - 94.09%
        - 220404180001_aspp0123:
                  - 94.25%
        - 220404180001_gtk3:
                  - 94.14%
        - 220404180001_layernorm:
                  - 91.53%
        - 220404180001_sharedgproj:
                  - 94.13%
        - 220404180001_dimx025:
                  - 87.76%
        - 220404180001_dimx05:
                  - 91.84%
        - 220404180001_dimx1:
                  - 94.48%
        - 220404180001_dimx2:
                  - 94.82%
        - 220404180001_dimx3:
                  - 94.86%
        - 220404180001_dimx1_epoch60:
                  - 93.66%
        - 220404180001_dimx1_epoch60_bsx2:
                  - 92.11%
        - 220404180001_dimx1_epoch480_bsx025:
                  - 87.89%
        - 220404180001_gprojdim512:
                  - 60 epochs
                  - 93.28%
        - 220404180001_gprojdim512_rerun:
                  - 94.11%
        - 220404180001_gprojdim128:
                  - 94.28%
        - 220404180001_gprojdim64:
                  - 94.18%

        # joints 15
        - 220405153001:
                  - 90.67%

sgn_v5:
        - 220407140001_part1_pt1_mot3_parttype1:
                  - part type 1 has hands and arms only
                  - 94.10%
        - 220407140001_part1_pt1_motion1_parttype2:
                  - part type 2 is the difference between the L/R arms and legs
                  - 93.76%
        - 220407140001_part1_pt1_motion3_parttype2:
                  - 94.08%
        - 220407140001_position0_part1_pt1_motion3_parttype2:
                  - no xyz inputs, only difference between the L/R arms and legs
                  - 79.13%
        - 220407140001_jointtype1:
                  - uses add instead of concat for jt & dr
                  - 93.69%
        - 220407140001_nogshared:
                  - 94.20%
        - 220407140001_agcnnopaddata:
                  - only validated with the first 100 samples....
                  - 95.00%
        - 220407140001_agcnnopaddata_rerun:
                  - 93.64%
        - 220410210001_centerff:
                  - 94.46%

        # joints 15
        - 220410210001:
                  - 93.28%

sgn_v6:
        - 220411120001_gcntkernel3:
                  - 94.14%
        - 220411120001_gcntkernel3_tkernel0:
                  - 92.92%
        - 220411120001_gcntkernel3_fipregcn:
                  - 93.83%
        - 220411120001_gcntkernel3_tkernel0_fipregcn:
                  - 92.86%
        - 220411220001_fipregcn:
                  - 94.20%
        - 220411120001_all64dim:
                  - 87.61%
        - 220411120001_cmulti1105025:
                  - 92.26%
        - 220411120001_cmulti11105:
                  - 93.89%
        - 220411220001_motionsampler1_motionnorm1:
                  - 94.17%
        - 220411220001_par_pos_fusion1:
                  - 93.94%
        - grid_cmulti:
                  - order of dim priority (high to low) = c3,c4,c1,c2
        - grid_seg:
                  - tested sampling freq (seg) from 5 to 50
                  - 20 is the best

sgn_v7:
        # dropouts
        - 220413153001_dropout2d00:
                  - 94.14%
        - 220413153001_dropout2d01:
                  - 94.49%
        - grid_dropout:
                  - up until 0.2 all is above 94 and then all below.

        # position and parts fusion
        - 220413153001_par1_mot1_pt2_parposfus0:
                  - sem part 1
                  - pre spatial gcn
                  - 94.08%
        - 220413153001_par1_mot1_pt2_parposfus1:
                  - sem part 1
                  - post spatial gcn
                  - dual GCN
                  - 94.03%
        - 220413153001_par1_mot1_pt2_spar1_parposfus1_gpart1:
                  - post spatial gcn
                  - no gcn for parts
                  - 93.68%

        - 220413153001_par1_mot1_pt2_spar1_parposfus2:
                  - pre spatial gcn
                  - 1 MLP after fusion
                  - 94.19%
        - 220413153001_par1_mot1_pt2_spar1_parposfus3:
                  - post spatial gcn
                  - dual GCN
                  - 1 MLP after fusion
                  - 93.94%
        - 220413153001_par1_mot1_pt2_spar1_parposfus3_gpart1:
                  - post spatial gcn
                  - no gcn for parts
                  - 1 MLP after fusion
                  - 93.90%

        - 220413153001_par1_mot1_pt2_spar1_parposfus4:
                  - pre spatial gcn
                  - 2 MLP after fusion
                  - 93.89%
        - 220413153001_par1_mot1_pt2_spar1_parposfus5:
                  - post spatial gcn
                  - dual GCN
                  - 2 MLP after fusion
                  - 93.77%
        - 220413153001_par1_mot1_pt2_spar1_parposfus5_gpart1:
                  - post spatial gcn
                  - no gcn for parts
                  - 2 MLP after fusion
                  - 93.99%
        - 220414230001_par1_mot1_pt0_spar1_parposfus5_gpart1:
                  - 94.36%

        # t modes
        - 220413153001_tmode2:
                  - temporal cnn with residual
                  - 94.23%
        - 220413153001_tmode2_cmulti10101005:
                  - temporal cnn with residual
                  - reduced c4
                  - 94.03%
        - 220413153001_tmode2_cmulti101005025:
                  - temporal cnn with residual
                  - reduced c3 c4
                  - 92.59%
        - 220413153001_tmode3:
                  - temporal cnn with all 3x3
                  - 94.53%
        - 220413153001_tmode5:
                  - temporal cnn with 3convs
                  - 94.20%
        - 220414230001_tmode7:
                  - temporal cnn with all layers having dropouts
                  - 93.51%
        - 220414230001_tmode7_gcndropout02:
                  - temporal cnn with all layers + gcn with dropouts
                  - 91.38%
        - 220414230001_tmode7_gcndropout02_pos4_vel4:
                  - temporal cnn with all layers + gcn + input emb with dropouts
                  - 90.49%
        - 220414230001_tmode8:
                  - temporal cnn with all layers having dropouts + residual
                  - 93.97%

        # residuals
        - 220413153001_gres111:
                  - residual in the gcn blocks
                  - 93.80%
        - 220413153001_gres111_nogshared:
                  - 94.08%
        - 220413153001_gres111_cmulti10101005:
                  - resifual in GCN
                  - reduced c4
                  - 93.73%
        - 220413153001_gres111_cmulti101005025:
                  - resifual in GCN
                  - reduced c3 c4
                  - 92.25%

        # no smp
        - 220414210001_smp0:
                  - 93.32%

        - 220415110001_pos4_vel4_gcndrop_tmode7_alldropouts:
                  - 90.49%
        - 220415110001_pos5_vel5:
                  - input has inverted bottleneck like ffn in transformers.
                  - 94.36%
        - 220415110001_pos6_vel6_gres111_tmode2:
                  - input + gcn + temporal cnn all residual
                  - 94.02%

sgn-code:
        - original sgn repo code
        - 220324170001:
                  - 94.5%
        - 220324170001_rerun:
                  - 94.2%
        - 220324170001_rerun2:
                  - 94.2%
        - 220324170001_rerun3:
                  - 94.2%
        - 220324170001_rerun4:
                  - 94.2%
        - 220324170001_rerun5:
                  - 94.2%
        - 220324170001_rerun6_16workers:
                  - 94.5%
        - 220324170001_rerun7_32workers:
                  - 94.0%
