aagcn:
        - 10 layers:
                  - original, old dataset generation:
                            - 94.7%
                            - overfit, train acc ~1
                  - original, 4gpu:
                            - 94.5%
                            - overfit, train acc ~1
                  - original:
                            - 94.3%
                            - overfit, train acc ~1
                  - no attention:
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
                  - no pad:
                            - 94.3%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.3%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.8-1.2):
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.5-1.5):
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
        - 6 layers:
                  - original:
                            - 94.4%
                            - overfit, train acc ~1
                            - almost identical to the 10 layers.
                  - no attention:
                            - 93.8%
                            - overfit, train acc ~1
                  - no pad:
                            - 94.1%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
        - 3 layers:
                  - original:
                            - 93.2%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no attention:
                            - 92.6%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad:
                            - 93.1%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad, no attention:
                            - 92.5%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1

aagcn_v2:
        - A does not depend on predefined A matrix. > no noticable difference.
        - Using 1 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1
        - Using 3 subset:
                  - no noticable difference. 94.1%
                  - overfit, train acc ~1
        - Using 5 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1

aagcn_v3:
        - Added additional projection in the GCN.
        - no noticable changes. 94.6%
        - overfit, train acc ~1

aagcn_v4:
        - Merged TCN into GCN.
        - Create different sets of subsets.
        - Each set will have different kernel size in temporal dim. (1,3,6,9)
        - did not converge

aagcn_v5:
        - added se block after TCN, sct attention is removed:
                  - slightly worse 93.72%
                  - overfit, train acc ~1
        - without the tse block, sct attention is removed.

aagcn_v6:
        - removed TCN. Added tcn conv in AdaptiveGCN (conv_d)
        - slightly worst 93%
        - overfit, train acc ~1

aagcn_v7:
        - removed TCN, added TSE in the AdaptiveGCN.
        - worst, 92.1%

aagcn_v8:
        - creates multiple attentions instead of one in AdaptiveGCN
        - using 5 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1
        - using 3 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1

aagcn_v9:
        - uses lstm before classification.
        - GAP-TV, proj4:
                  - 93.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 6layers:
                  - 92.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 3layers:
                  - 91.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, bi:
                  - 93.7%
                  - overfit, faster than original model, train acc ~1
                  - not much difference compared to lstm

aagcn_v10:
        - uses MHA before classification.
        - GAP-TV, 1h:
                  - 92%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h:
                  - 92.4%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 6layers:
                  - 91.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 3layers:
                  - 91.5%
                  - overfit, faster than original model, train acc ~1
        - GAP-T, 1h:
                  - 91.2%
                  - overfit, slower than original model, train acc ~1
        - GAP-T, 2h:
                  - 91%
                  - overfit, slower than original model, train acc ~1
        - Flat, 2h:
                  - 84.2%
                  - No Overfitting

aagcn_v11:
        - uses transformer layers with projection
        - COMPARED TO aagcn_v13:
                  - feature projection with increasing dim and striding is causing the non convergence.
        - 211217153001_cls_pos_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_cls_mt_vc_2h_1a:
                  - did not converge
        - 211217153001_gap_mt_vc_2h_1a:
                  - did not converge
        - 211218220001_pos_gap_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_1a_3l_dropout_noproj_torchtrans:
                  - did not converge
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_torchtrans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211218220001_pos_cls_mt_vc_2h_3a_1l_dropout_noproj_lowerlr:
                  - 88.9%
                  - overfit, train acc ~1

aagcn_v12:
        - uses attention learned from conv. (unary)
        - 211216170001, with relu for attention:
                  - 90.0 %.
                  - did not overfit, may need longer training. !!!!!
        - 211216170001_tanh, with tanh for attention:
                  - 94.2%
                  - almost identical to the original.

aagcn_v13:
        - uses original transformer layers + 1 aagcn projection block.
        - 211219110001_pos_cls_3trans:
                  - ~70%
                  - converge but training was unstable.
        - 211219110001_pos_cls_3trans_lowerlr:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm_64dim:
                  - 89.4%
                  - overfit, train acc ~1
        - 211219110001_cls_3trans_lowerlr_prenorm:
                  - 89.3%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_3trans_lowerlr_prenorm:
                  - 89.7%
                  - overfit, train acc ~1
        - 211219110001_pos_cls_5trans_lowerlr_prenorm:
                  - 89.9%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_8trans_lowerlr_prenorm:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm:
                  - 90.2%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_10trans_lowerlr_prenorm_nopad:
                  - 90.6%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_15trans_lowerlr_prenorm:
                  - 89.8%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.4%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm_noattn:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220100001_3base_pos_cls_3trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211220100001_2base_pos_cls_3trans_lowerlr_prenorm:
                  - 89%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_4h:
                  - 89.7%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_8h:
                  - 90%
                  - overfit, train acc ~1
        - 211220100001_pos_cls_3trans_lowerlr_prenorm_1h:
                  - 89.3%
                  - overfit, train acc ~1
        - 211220230001_0base_pos_cls_3trans_lowerlr_prenorm:
                  - 81.3%
                  - did not overfit.
        - 211220230001_0base_pos_cls_5trans_lowerlr_prenorm:
                  - 84%
                  - did not overfit.
        - 211220230001_0base_pos_cls_8trans_lowerlr_prenorm:
                  - 83.9%
                  - did not overfit.
        - 211220230001_0base_pos_cls_10trans_lowerlr_prenorm:
                  - 83.1%
                  - did not overfit.
        - 211221113001_pos_gap_3trans_lowerlr_prenorm:
                  - 88.8%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_5trans_lowerlr_prenorm:
                  - 89.2%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_8trans_lowerlr_prenorm:
                  - 90.0%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm:
                  - 89.5%
                  - overfit, train acc ~1
        - 211221113001_pos_gap_10trans_lowerlr_prenorm_nopad:
                  - 90.8%
                  - overfit, train acc ~1
aagcn_v14:
        - uses original transformer to replace tcn
        - 211221123001_pos_1trans_3l_prenorm:
                  - 84.8%
                  - overfit, train acc ~1
aagcn_v16:
        - puts positional encoder into tcn
        - 211221140001:
                  - 93.9%
                  - overfit, train acc ~1
aagcn_v17:
        - change tcn to create patches. (no overlap during conv)
        - uses transformer - temporal
        - 211228140001_nopad:
                  - diverge
        - 211228140001_nopad_3ks:
                  - 87%
                  - almost overfit, train acc ~1
        - 211228140001_nopad_10ks:
                  - diverge
        - 211228140001_nopad_3ks_5trans:
                  - 80.4%
                  - almost overfit, train acc ~1
                  - only converged midway
        - 211228140001_nopad_3ks_adam:
                  - diverged
        - 211228140001_nopad_3ks_adam_0_01lr:
                  - diverged
        - 211228140001_nopad_3ks_64dim:
                  - diverged
        - 211228140001_nopad_5ks_64dim:
                  - diverged
        # COSSIN PE -----------------------------------------
        - 220111210001_nopad_3ks_cossin:
                  - diverged
        - 220111210001_nopad_3ks_cossin_lowerlr:
                  - 89.2%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_noaug_lowerlr:
                  - 90.7%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_cossin_100div_noaug:
                  - converged at warmup but diverged afterwards.
        - 220111210001_nopad_3ks_cossin_100div_lowerlr:
                  - 87.5%
                  - overfit, train acc ~1
        - 220125120001_nopad_3ks_lowerlr_100div_cossin_rerun_randommoveshift:
                  - 89.5%
                  - almost overfit, train acc ~1
        - ...: # 220125120001_nopad_3ks_lowerlr_100div_cossin_rerun_randommoveshift_drop_out:
                  - 89.5%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_8d_25h:
                  - 90.0%
        - 220111210001_nopad_3ks_cossin_100div_noaug_lowerlr: #####
                  - 90.5%
                  - overfit, train acc ~1
                  - converged really fast at the beginning (val acc was higher)
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_25h:
                  - 91.2%
        - 220209100001_nopad_3ks_noaug_lowerlr_100div_cossin_25h_6l:
                  - 91.0%
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_32d:
                  - 90.4%
                  - almost overfit, train acc ~1
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_64d:
                  - 90.5%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_64d_25h:
                  - 91.2%
        - 220121120001_nopad_3ks_noaug_lowerlr_100div_cossin_128d:
                  - 89.7%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_128d:
                  - 89.7%
        - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_4h:
                  - did not converge.
        - 220201210001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_8h:
                  - 91.3%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_16h:
                  - 91.5%
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_32d: (0.5dropout)
                  - 88.9%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_64d: (0.5dropout)
                  - 85.0%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_128d: (0.5dropout)
                  - 87.6%
                  - almost overfit, train acc ~1
        - 220118230001_nopad_3ks_noaug_lowerlr_100div_cossin_256d: (0.5dropout)
                  - GPU mem error
        # RERUN WITH POS PE -----------------------------------------
        - 220111210001_nopad_3ks_rerun:
                  - same as 211228140001_nopad_3ks
                  - 87%
                  - almost overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_lowerlr:
                  - 84.6%
                  - overfit, train acc ~1
        - 220111210001_nopad_3ks_rerun_noaug:
                  - 77.8%
                  - only converged midway
        - 220111210001_nopad_3ks_rerun_noaug_lowerlr:
                  - 88.2%
                  - converged really fast at the beginning (val acc was higher)
        # ATTENTION MASKING -----------------------------------------
        - 220112190001_nopad_3ks_noaug_lowerlr_attnmask_softmask_testing:
                  - uses input frame mask
                  - uses attention masking => doest seem to have a diff
                  - 89.0%
                  - almost overfit, train acc ~1
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
                  - only have access to past info.
                  - 89.2%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
                  - only have access to future info.
                  - 90.5%

        # - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
        #           - buggy, wrong logic
        #           - 89.3%
        # - 220202230001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
        #           - buggy, wrong logic
        #           - 90.5%
        # - 220112190001_nopad_3ks_noaug_lowerlowerlr_attnmask_forward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - uses forward masking.
        #           - masking starts after warmup epochs.
        #           - goes up during warmup epochs and decreases afterwards.
        #           - 24.4%
        # - 220125120001_nopad_3ks_noaug_lowerlr_100div_cossin_forward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - uses forward masking in last layer.
        #           - 89.1%
        #           - almost overfit, train acc ~1
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_backward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - backward masking
        #           - 90.5%
        #           - almost overfit, train acc ~1
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_backward_128d_8h:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - no backward
        #           - 91.3%
        # - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_8h_backward:
        #           - BUGGY MASKING NOT APPLIED AT EVAL
        #           - 91.3%
        # - 220201210001_..._128d_8h_backward_rerun:
        #           - - BUGGY MASKING NOT APPLIED AT EVAL
        #           - 91.3%

        # NO RELU IN TCNGCN UNIT -----------------------------------------
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu:
                  - no relu after tcngcn unit.
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin:
                  - no relu after tcngcn unit.
                  - 90.2%
                  - almost overfit, train acc ~1
        - 220117110001_nopad_3ks_noaug_lowerlr_notcngcnrelu_100div_cossin_mul:
                  - no relu after tcngcn unit.
                  - 88.6%
                  - almost overfit, train acc ~1
        # SAM ---------------------------------------------------------------
        - 220125120001_nopad_3ks_noaug_lowerlr_100div_cossin_sam:
                  - 90.5%
                  - almost overfit, train acc ~1
        # ADAM ---------------------------------------------------------------
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_adam:
                  - 52.9%
        # LN for data_norm -------------------------------------------------
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_ln:
                  - 89.2%
                  - almost overfit, train acc ~1
        # DROPOUT ----------------------------------------------------
        - 220112190001_nopad_3ks_noaug_lowerlr_dropout05:
                  - 86.2%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_nodropout:
                  - 89.1%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout01:
                  - 89.9%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout03:
                  - 90.4%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout04:
                  - 89.8%
                  - almost overfit, train acc ~1
        - 220118160001_nopad_3ks_noaug_lowerlr_100div_cossin_dropout05:
                  - 89.0%
                  - almost overfit, train acc ~1
        # PROJ -------------------------------------
        - 220121110001_nopad_1ks_noaug_lowerlr_100div_cossin:
                  - 90.2%
                  - almost overfit, train acc ~1
        - 220121110001_nopad_2ks_noaug_lowerlr_100div_cossin:
                  - 90.6%
                  - almost overfit, train acc ~1

        # more gpu to reduce the batch size per gpu -----------------------
        - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_4gpu:
                  - 90.2%
                  - almost overfit, train acc ~1

        - 220127170001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_2h:
                  - 90.4%
                  - almost overfit, train acc ~1

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_onecycliclr:
                  - 88.6%
                  - with backward
        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_32d_cycliclr:
                  - 88.6%
                  - with backward

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_128d_dropout05:
                  - 89.2%

        - 220128130001_nopad_3ks_noaug_lowerlr_100div_cossin_nodecay:
                  - decay = 5e4 *******
                  - 88.9%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_higherwd:
                  - 1e-3 decay
                  - 88.9%

        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_16dffn:
                  - 89.6%
        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_256dffn:
                  - 91.0%

        - 220207140001_nopad_3ks_noaug_lowerlr_100div_cossin_stretch:
                  - remove zero padding and stretch the original values.
                  - 31.1%

aagcn_v18:
        - added shift operation after the gcn. This forward concats the features.
        - 220103110001_nopad:
                  - diverged
        - 220103110001_nopad_5ks:
                  - 73.1%
                  - almost overfit, train acc ~1

aagcn_v19:
        - dual transformer in helix form. spatial + temporal, 2 PE.
        - 220112150001_nopad_3ks:
                  - diverged
        - 220112150001_nopad_3ks_cossin:
                  - diverged
        - 220112150001_nopad_3ks_lowerlr_noaug:
                  - 89.5%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA:
                  - 90.0%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_lowerlr_noaug_PA_96d:
                  - 89.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_PA:
                  - diverged
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr:
                  - 83.9%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA:
                  - 85.3%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug:
                  - 89.7%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - 89.9%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

        - attn_220112150001_nopad_3ks_nosplittingclstoken_lowerlr_PA_noaug:
                  - spatial attention seems to be work as intended for some cases.
                  - it is always highly influenced by J1
                  - there are cases where it is attending "correctly" but false prediction
                  - i.e. 43-7, 60, 1

        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlowerlr_noaug:
                  - 85.4%
                  - almost overfit, train acc ~1
        - 220112150001_nopad_3ks_nosplittingclstoken_lowerlr_noaug_shorterlr:
                  - 89.3%
                  - almost overfit, train acc ~1
                  - converged really fast at the beginning

aagcn_v20:
        - added dual transformer, and merge at the end.
        - 220114130001_nopad_3ks_lowerlr_noaug:
                  - 87.1%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin:
                  - 88.7%
                  - almost overfit, train acc ~1
        - 220114130001_nopad_3ks_lowerlr_noaug_cossin_64d:
                  - 89.5%
                  - almost overfit, train acc ~1

aagcn_v21:
        - positional masking instead of PE at input.
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_Nonedp:
                  - 88.3%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_0dp:
                  - 88.1%
                  - almost overfit, train acc ~1
        - 220117150001_nopad_3ks_noaug_lowerlr_8dp:
                  - 88.4%
                  - almost overfit, train acc ~1

aagcn_v22:
        - adds pe to each of the layers. v17
        - 220117190001_nopad_3ks_noaug_lowerlr:
                  - 89.1%
                  - almost overfit, train acc ~1
        - 220117190001_nopad_3ks_noaug_lowerlr_cossin:
                  - 90.4%
                  - almost overfit, train acc ~1

aagcn_v23:
        - adds pe to each of the layers. v20
        - 220117210001_nopad_3ks_lowerlr_noaug:
                  - 87.7%
                  - almost overfit, train acc ~1
        - 220117210001_nopad_3ks_lowerlr_noaug_cossin:
                  - 89.1%
                  - almost overfit, train acc ~1

aagcn_v24:
        - only spatial transformer
        - 220118110001_nopad_3ks_noaug_lowerlr:
                  - 65.1%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_32d:
                  - 72.2%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_64d:
                  - 75.2%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_128d:
                  - 80.9%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_256d:
                  - 83.8%
                  - no overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin:
                  - 62.7%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_32d:
                  - 71.4%
                  - no overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_64_cossind:
                  - 75.6%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d: ####
                  - 84.3%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_256d:
                  - 83.7%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_5l:
                  - 84.1%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_7l:
                  - 83.1%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_4h:
                  - 82.2%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_8h:
                  - 82.6%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_16h:
                  - 83.9%
                  - almost overfit

        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_2aagcn:
                  - 83.3%
                  - almost overfit
        - 220118110001_nopad_3ks_noaug_lowerlr_cossin_128d_3aagcn:
                  - 80.3%
                  - almost overfit

        # - 220125170001_nopad_3ks_noaug_lowerlr_cossin_16d:
        #           - FAULTY, NO CLS TOKEN
        #           - cls masking
        #           - 60.9%
        # - 220125170001_nopad_3ks_noaug_lowerlr_cossin_128d:
        #           - FAULTY, NO CLS TOKEN
        #           - cls masking
        #           - 71.7%

        # - 220126100001_nopad_3ks_noaug_lowerlr_cossin_PAa:
        #           - FAULTY, NO CLS TOKEN
        #           - 59.9%
        #           - added PAa as attn mask.
        #           - with CLS_MASK
        # - 220126100001_nopad_3ks_noaug_lowerlr_cossin_3PAa:
        #           - FAULTY, NO CLS TOKEN
        #           - 66.9%
        #           - added 3 diff PAa as attn mask.
        #           - with CLS_MASK

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_CLS_MASK:
                  - with CLS_MASK
                  - 19.2%

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_PAa_144d:
                  - added 3 diff PAa as attn mask.
                  - 82.3%
                  - almost overfit
        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_PAa_144d_CLS_MASK:
                  - added 3 diff PAa as attn mask.
                  - with CLS_MASK
                  - 72.8%

        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_3PAa_144d_CLS_MASK:
                  - added 3 diff PAa as attn mask.
                  - with CLS_MASK
                  - 73.3%
        - 220127220001_nopad_3ks_noaug_lowerlr_cossin_3PAa_144d:
                  - added 3 diff PAa as attn mask.
                  - 82.3%
                  - almost overfit

aagcn_v25:
        - only temporal transformer with tcn proj
        - 220121103001_nopad_3ks_noaug_lowerlr:
                  - 89.5%
                  - almost overfit
        - 220121103001_nopad_3ks_noaug_lowerlr_cossin:
                  - 89.6%
                  - almost overfit

aagcn_v26:
        - only spatial transformer with tcn proj
        - 220121100001_nopad_3ks_noaug_lowerlr:
                  - 54.1%
        - 220121100001_nopad_3ks_noaug_lowerlr_cossin:
                  - 53.5%

aagcn_v27:
        - spatial transformer, deberta
        - 220124163001_nopad_3ks_noaug_lowerlr:
                  - 62.8%
        - 220124163001_nopad_3ks_noaug_lowerlre4:
                  - 2.73%
                  - did not converge
        - 220124163001_nopad_3ks_noaug_lowerlre5_adam:
                  - 6.23%
                  - did not converge

aagcn_v28:
        - temporal transformer, deberta
        - 220124183001_nopad_3ks_noaug_lowerlr:
                  - 89.7%
                  - almost overfit
        - 220124183001_nopad_3ks_noaug_lowerlre4:
                  - 73.4%
                  - almost overfit
        - 220124183001_nopad_3ks_noaug_lowerlre5_adam:
                  - 77.1%
                  - almost overfit
        - 220126153001_nopad_3ks_noaug_lowerlr_emd:
                  - 89.2%
                  - almost overfit
        - 220126153001_nopad_3ks_noaug_lowerlr_emd_128d:
                  - 90.0%
                  - almost overfit
        - 220207153001_nopad_3ks_noaug_lowerlr_emd_128d_8h:
                  - 91.1%
        - 220201213001_nopad_3ks_noaug_lowerlr_emd_span3:
                  - 88.8%
        - 220201213001_nopad_3ks_noaug_lowerlr_emd_span10:
                  - 89.2%

aagcn_v29:
        - from 20, merge FM after S and T transformer in each layer.
        - 220125200001_nopad_3ks_lowerlr_noaug_cossin:
                  - mean was applied to spatial trans before classifier
                  - 88.7%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin:
                  - concat features from both trans before classifier
                  - 87.9%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_16_128:
                  - concat features from both trans before classifier
                  - 88.0%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_PA:
                  - concat features from both trans before classifier
                  - PA for spatial trans
                  - 87.8%
                  - almost overfit
        - 220126110001_nopad_3ks_lowerlr_noaug_cossin_PAa:
                  - concat features from both trans before classifier
                  - PA for spatial trans wit gate variable.
                  - 88.0%
                  - almost overfit

aagcn_v30:
        - uses trans for tcn and gcn for spatial.
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin:
                  - 87.3%
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin_agcnv2:
                  - 85.3%
        - 220128110001_nopad_3ks_lowerlr_noaug_cossin_64d:
                  - 89.4%

aagcn_v31:
        - based on aagcn_v19.
        - uses 1 PE.
        - sequential and alternates between temporal and spatial.
        - 220208220001_nopad_3ks_lowerlr_noaug_cossin:
                  - 89.1%
        - 220209120001_nopad_3ks_lowerlr_noaug_cossin_clstokensplit_newspatial:
                  - 89.8%
