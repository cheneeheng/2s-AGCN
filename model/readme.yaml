aagcn:
        - 10 layers:
                  - original, old dataset generation:
                            - 94.7%
                            - overfit, train acc ~1
                  - original, 4gpu:
                            - 94.5%
                            - overfit, train acc ~1
                  - original:
                            - 94.3%
                            - overfit, train acc ~1
                  - no attention:
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
                  - no pad:
                            - 94.3%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.3%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.8-1.2):
                            - 94.1%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
                  - original, zflip, xyaxisshift (0.5-1.5):
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention or with pad
        - 6 layers:
                  - original:
                            - 94.4%
                            - overfit, train acc ~1
                            - almost identical to the 10 layers.
                  - no attention:
                            - 93.8%
                            - overfit, train acc ~1
                  - no pad:
                            - 94.1%
                            - almost identical to the 10 layers.
                  - no pad, no attention:
                            - 94.0%
                            - overfit, train acc ~1
                            - not much difference compared to the one with attention
        - 3 layers:
                  - original:
                            - 93.2%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no attention:
                            - 92.6%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad:
                            - 93.1%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1
                  - no pad, no attention:
                            - 92.5%
                            - overfit, train acc ~1
                            - there is more noise in the train accuracy at ~1

aagcn_v2:
        - A does not depend on predefined A matrix. > no noticable difference.
        - Using 1 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1
        - Using 3 subset:
                  - no noticable difference. 94.1%
                  - overfit, train acc ~1
        - Using 5 subset:
                  - no noticable difference. 94.3%
                  - overfit, train acc ~1

aagcn_v3:
        - Added additional projection in the GCN.
        - no noticable changes. 94.6%
        - overfit, train acc ~1

aagcn_v4:
        - Merged TCN into GCN.
        - Create different sets of subsets.
        - Each set will have different kernel size in temporal dim. (1,3,6,9)
        - did not converge

aagcn_v5:
        - added se block after TCN, sct attention is removed:
                  - slightly worse 93.72%
                  - overfit, train acc ~1
        - without the tse block, sct attention is removed.

aagcn_v6:
        - removed TCN. Added tcn conv in AdaptiveGCN (conv_d)
        - slightly worst 93%
        - overfit, train acc ~1

aagcn_v7:
        - removed TCN, added TSE in the AdaptiveGCN.
        - worst, 92.1%

aagcn_v8:
        - creates multiple attentions instead of one in AdaptiveGCN
        - using 5 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1
        - using 3 splits:
                  - no noticable difference. 94.5%
                  - overfit, train acc ~1

aagcn_v9:
        - uses lstm before classification.
        - GAP-TV, proj4:
                  - 93.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 6layers:
                  - 92.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, 3layers:
                  - 91.9%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, proj4, bi:
                  - 93.7%
                  - overfit, faster than original model, train acc ~1
                  - not much difference compared to lstm

aagcn_v10:
        - uses MHA before classification.
        - GAP-TV, 1h:
                  - 92%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h:
                  - 92.4%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 6layers:
                  - 91.2%
                  - overfit, faster than original model, train acc ~1
        - GAP-TV, 2h, 3layers:
                  - 91.5%
                  - overfit, faster than original model, train acc ~1
        - GAP-T, 1h:
                  - 91.2%
                  - overfit, slower than original model, train acc ~1
        - GAP-T, 2h:
                  - 91%
                  - overfit, slower than original model, train acc ~1
        - Flat, 2h:
                  - 84.2%
                  - No Overfitting
