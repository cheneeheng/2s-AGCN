sgn_v11:
        - 220525170001_midvel1:
                  - 94.21%
        - 220525170001_midvel1_multit357:
                  - 94.82%

        - 220527120001_gcnweighted1:
                  - added previous G to the current G
                  - G not shared among the GCNs
                  - 94.61%

        - 220527120001_vkernel1:
                  - 94.60%
        - 220527120001_vkernel1_gcnffn1:
                  - NO FFN
                  - 94.60%
        - 220527120001_vkernel1_gcnffn1_rerun:
                  - 94.46%
        - 220527120001_vkernel1_gcnffn2:
                  - 93.67%
        - 220527120001_gnonshared_vkernel1_gcnffn1:
                  - 93.90%
        - 220527120001_gnonshared_vkernel1_gcnffn2:
                  - 93.80%
        - 220530120001_gcnffnres025:
                  - 94.67%
        - 220530120001_gcnffnres05:
                  - 94.55%
        - 220527120001_gcnffn1:
                  - ffn multiplier 1
                  - 94.35%
        - 220527120001_gcnffnres1:
                  - ffn multiplier 1
                  - residual addition in code
                  - 94.74%
        - 220527120001_gcnffn2:
                  - ffn multiplier 2
                  - 94.12%
        - 220527120001_gcnffnres2:
                  - ffn multiplier 2
                  - residual addition in code
                  - 94.62%

        - 220527120001_nogcnres:
                  - no residual in gcn (w2 = 0)
                  - 92.78%

        - 220520150001_rerun_orisgn:
                  - 94.56%
        - 220520150001_multit333:
                  - 95.14% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220520150001_multit357:
                  - 95.03%
        - 220525170001_multit357357357_cmulti11105:
                  - 95.08%
        - 220525170001_multit357357357shared_cmulti11105:
                  - 95.14%

        - 220520150001_gcnfpn0_multit333_pregcntemsem:
                  - no fpn
                  - 94.83%
        - 220520150001_gcnfpn0_multit555_pregcntemsem:
                  - 94.90%
        - 220520150001_gcnfpn0_multit357357357_pregcntemsem:
                  - 94.94%
        - 220601220001_gcnfpn0_multit357357357_allgcndim256:
                  - 94.98%
        - 220601220001_gcnfpn0_multit357357357shared_allgcndim256:
                  - 93.37%

        - 220520150001_gcnfpn1_multit333:
                  - proj and sum
                  - 95.12%
        - 220520150001_gcnfpn1_multit333shared:
                  - 95.12%
        - 220520150001_gcnfpn1_multit555:
                  - 95.25% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                  - best epoch was 94
        - 220520150001_gcnfpn1_multit555shared:
                  - 94.88%
        - 220520150001_gcnfpn1_multit777:
                  - 95.24%
        - 220520150001_gcnfpn1_multit777shared:
                  - 94.92%
        - 220520150001_gcnfpn1_multit357357357:
                  - 95.21%
        - 220520150001_gcnfpn1_multit357357357shared:
                  - 95.52% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220520150001_gcnfpn1_multit357935793579shared:
                  - 95.24%
        - 220520150001_gcnfpn1_aspp1357:
                  - 94.52%
        - 220520150001_gcnfpn1_aspp01357:
                  - 94.67%
        - 220520150001_gcnfpn1_multit371137113711shared:
                  - 95.37% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220601220001_gcnfpn7_multit357357357shared:
                  - same as fpn1 but with kernel3
                  - 95.66% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< BEST
        - 220601220001_gcnfpn7_multit357357357:
                  - 95.48% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220601220001_gcnfpn7_multit357:
                  - 95.61% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220607103001_gcnfpn1_multit357357357shared_fcmerge2:
                  - 95.22%
        - 220607103001_gcnfpn7_multit357357357shared_fcmerge2:
                  - 95.42%
        - 220607103001_gcnfpn7_multit357357357shared_tmode2:
                  - 95.42%
        - 220607103001_gcnfpn1k9_multit357357357shared:
                  - 95.50% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220607103001_gcnfpn1k7_multit357357357shared:
                  - 95.62% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220607103001_gcnfpn1k5_multit357357357shared:
                  - 95.62% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        - 220628140001_gcndim256256256_gcnfpn1k3_multit357357357shared:
                  - 95.34% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220616130001_gcnfpn1k3_multit357357357shared_tmp2:
                  - temporal max pool with indices
                  - 95.37% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220623113001_gcnfpn1k3_multit111111111shared:
                  - 95.28% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220630190001_gcnfpn1k3_multit357357357shared_semcls1:
                  - 92.06%

        - 220520150001_gcnfpn2_multit333:
                  - proj to lower and sum
                  - 94.78%
        - 220520150001_gcnfpn2_multit333shared:
                  - 94.02%
        - 220520150001_gcnfpn2_multit555:
                  - 94.72%
        - 220520150001_gcnfpn2_multit357357357:
                  - 95.11%
        - 220520150001_gcnfpn2_multit357357357shared:
                  - 95.05%
        - 220603153001_gcnfpn2kernel3_multit357357357:
                  - 95.30%
        - 220603153001_gcnfpn2kernel3_multit357357357shared:
                  - 95.35%

        - 220520150001_gcnfpn3_multit333:
                  - proj
                  - 94.94%
        - 220520150001_gcnfpn3_multit333shared:
                  - 94.44%
        - 220520150001_gcnfpn3_multit555:
                  - 94.95%
        - 220520150001_gcnfpn3_multit555shared:
                  - 94.97%
        - 220520150001_gcnfpn3_multit777:
                  - 95.07%
        - 220520150001_gcnfpn3_multit777shared:
                  - 94.94%
        - 220520150001_gcnfpn3_multit357357357shared:
                  - 95.09%
        - 220520150001_gcnfpn3_multit357357357:
                  - 95.23% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        - 220525170001_gcnfpn4_multit357:
                  - proj and concat
                  - 95.17%
        - 220525170001_gcnfpn4_multit3:
                  - 95.32% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        - 220525170001_gcnfpn5_multit357_cmulti111025_notmp:
                  - proj to lower and concat
                  - 94.59%
        - 220525170001_gcnfpn5_multit357_cmulti111025:
                  - 94.61%

        - NO temporal_maxpool:
                  - 220525170001_gcnfpn6_multit357___cmulti1111:
                            - 93.09%
                  - 220525170001_gcnfpn6_multit357___cmulti11105:
                            - 93.09%
                  - 220525170001_gcnfpn6_multit357___cmulti111025:
                            - 93.25%
                  - 220525170001_gcnfpn6_multit357357357shared_cmulti1111:
                            - 93.78%
                  - 220525170001_gcnfpn6_multit357357357shared_cmulti11105:
                            - 93.53%
                  - 220525170001_gcnfpn6_multit357357357shared_cmulti111025:
                            - 93.70%
                  - 220525170001_gcnfpn6_multit357357357_cmulti111025:
                            - 93.74%
                  - 220525170001_gcnfpn6_multit357357357_cmulti11105:
                            - 93.95%
                  - 220525170001_gcnfpn6_multit357357357_cmulti1111:
                            - 93.37%

        - 220527120001_gcnfpn6_multit357357357shared_cmulti111025:
                  - proj to 64 and sum
                  - 93.34%
        - 220527120001_gcnfpn6_multit357357357shared_cmulti11105:
                  - 93.89%
        - 220527120001_gcnfpn6_multit357357357shared_cmulti1111:
                  - 93.99%

        - 220603153001_gcnfpn83layer_multit357357357shared:
                  - bifpn, 64dim, 3 layers
                  - 93.75%
        - 220603153001_gcnfpn8_multit357357357shared:
                  - bifpn, 64dim, 1 layers
                  - 94.04%
        - 220603153001_gcnfpn8_multit357357357:
                  - bifpn, 64dim, 1 layers
                  - 94.13%
        - 220607103001_gcnfpn8dim128_multit357357357shared:
                  - 95.05% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220607103001_gcnfpn8dim128layer3_multit357357357shared:
                  - 94.55%
        - 220607103001_gcnfpn8dim256_multit357357357shared:
                  - 91.83%
        - 220607103001_gcnfpn8dim256layer3_multit357357357shared:
                  - 91.83%
        - 220607103001_gcnfpn8dim256layer3_multit357357357shared_fcmerge2:
                  - 82.26%
        # multiple kernels for fpn
        - 220628140001_gcndim256256256_gcnfpn9k357_multit357357357shared:
                  - multiple kernels for fpn
                  - 91.54%
        - 220628140001_gcndim256256256_gcnfpn9k357shared_multit357357357shared:
                  - 90.48%

        # TMODE = 3
        - 220613180001_tmode3_d256h4ffn64relu:
                  - gcnfpn1k3_multit357357357shared
                  - 94.32%
        - 220613180001_tmode3_d256h4ffn1024relu:
                  - gcnfpn1k3_multit357357357shared
                  - 94.42%
        - 220614140001_gcnfpn1k3_multit357357357shared:
                  - _tmode3_d512h4ffn128relu2l
                  - 94.06%
        - 220614140001_gcnfpn1k3_multit357357357shared:
                  - _tmode3_d512h4ffn2048relu2l
                  - 91.84%

        - 220616130001_gcnfpn1k3_multit357357357shared:
                  - _tmode3_d256h1ffn256relu2l
                  - 94.46%
        - 220616130001_gcnfpn1k3_multit357357357shared:
                  - _tmode3_d256h1ffn1024relu2l7
                  - 94.50%
        - 220616130001_gcnfpn1k3_multit357357357shared_tmp0_tmode3:
                  - _d128h1ffn128relu2l
                  - no temporal maxpool
                  - 94.21%
        - 220616130001_gcnfpn1k3_multit357357357shared_tmp0_tmode3:
                  - _d128h1ffn512relu2l
                  - no temporal maxpool
                  - 94.03%
        - 220616130001_gcnfpn1k3_multit357357357shared_tmp0_tmode3:
                  - _d256h1ffn256relu2l
                  - no temporal maxpool
                  - 94.52%
        - ? 220617113001_gcnfpn1k3_multit357357357shared_tmode3ffn256out512layer2bn
          : - 95.08%
        - 220617113001_gcnfpn1k3_multit357357357shared_tmode3ffn256layer2bn:
                  - 94.56%
        - 220617113001_gcnfpn1k3_multit357357357shared_tmode3ffn1024layer2bn:
                  - 94.59%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out1024layer2bn
                  - 95.31% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn512out1024layer2bn
                  - 95.05%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out2048layer2bn
                  - 95.19%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn128out1024layer2bn
                  - 95.21% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn64out1024layer2bn
                  - 94.64%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3dim256512ffnx1out1024layer2bn
                  - 94.45%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3dim256512ffnX2out1024layer2bn
                  - 94.51%
        - 220620113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3dim256512ffnX4out1024layer2bn
                  - 94.46%
        - 220620113001_gcndim128256512_gproj512_gcnfpn1k3:
                  - _multit357357357shared_tmode3ffn512out1024layer2bn
                  - 94.64%
        # 1 layer of MHA
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out512layer1bn
                  - 94.69%
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out1024layer1bn
                  - 95.27% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out2048layer1bn
                  - 94.92%
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out1024layer1bnh1d512
                  - 95.09%
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out1024layer1bnh2d512
                  - 95.25% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220623113001_gcnfpn1k3_multit357357357shared:
                  - _tmode3ffn256out1024layer1bnh2d256
                  - 95.30% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        # MORE GCNS + higher proj dim
        - 220616130001_gcndim128128256256_gcnfpn1k3_multit357357357shared:
                  - 95.63% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        - 220616130001_gproj512_gcnfpn1k3_multit357357357shared:
                  - 95.66% <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        - 220804140001_sgnori_smpemb_semj0_c1x2:
                  - 93.60%
        - 220804140001_sgnori_smpemb_semj0:
                  - 93.51%
        - 220804140001_sgnori_smpemb:
                  - 94.65%

        - 220804140001_sgnori_tmode3_bottleneck:
                  - 94.17%
        - 220804140001_sgnori_tmode3:
                  - 94.26%
        - 220809140001_sgnori_tmode3_1layer:
                  - 94.68%

        - 220810140001_sgnori_nogcnres:
                  - 92.48%
        - 220810140001_sgnori_128bs:
                  - 94.83%
        - 220810140001_sgnori_sgcnattn1:
                  - 93.34%
        - 220810140001_sgnori_sgcnattn2:
                  - 94.20%
        - 220810140001_sgnori_sgcnattn3:
                  - 93.64%
        - 220812150001_sgnori_sgcnattn10:
                  - 93.89%

        - 220812150001_sgnori_gsigmoid:
                  - 89.77%
        - 220812150001_sgnori_nogcnres_nosharedg:
                  - 92.08%

        - 220816140001_sgnori_inpos12_invel12:
                  - emb layers use 3, 3 kernels
                  - 94.22%
        - 220816140001_sgnori_inpos11_invel11:
                  - emb layers use 1, 3 kernels
                  - 94.46%

        - 220819140001_sgnori_inch2:
                  - 90.41%

        - 220830130001_sgnori_6gcn:
                  - 94.69%

        - 220901130001_sgnori_flgamma1:
                  - 94.72%
        - 220901130001_sgnori_flgamma2:
                  - 94.51%
        - 220901130001_sgnori_flgamma3:
                  - 94.20%
        - 220901130001_sgnori_flgamma4:
                  - 93.92%
        - 220901130001_sgnori_flgamma5:
                  - 93.55%
        - 220901130001_sgnori_flgamma01:
                  - 83.50%
        - 220901130001_sgnori_flgamma001:
                  - 94.76%
        - 220901130001_sgnori_flgamma0001:
                  - 94.75%

sgn_v12:
        - this has the infogcn MMD-based loss
        - G ACTIVATION IS IDENTITY HERE:
                  - 220706103001_bs64_adam_lr1e3:
                            - 89.06%
                  - 220706103001_bs128_adam_lr1e3:
                            - 90.42%
                  - 220706103001_bs128_adam_lr1e2:
                            - 74.98%
                  - 220706103001_bs128_adam_lr1e1:
                            - 1.65%
                  - 220706103001_bs128_sgd_lr1e3:
                            - 59.49%
                  - 220706103001_bs128_sgd_lr1e2:
                            - 86.83%
                  - 220706103001_bs128_sgd_lr1e1:
                            - 90.98%
                  - 220712163001_bs128_adam_lr1e3_lambda21e0:
                            - 89.83%
                  - 220712163001_bs128_adam_lr1e3_gain1:
                            - 89.92%
                  - 220712163001_bs128_sgd_lr1e1_steps90110:
                            - 91.43%
                  - 220714183001_bs128_sgd_lr1e1_steps90110_noise09:
                            - 1.65%
                  - 220714183001_bs128_sgd_lr1e1_steps90110_noise05:
                            - 1.65%
                  - 220714183001_bs128_sgd_lr1e1_steps90110_gain10:
                            - 91.40%
                  - 220714183001_bs128_sgd_lr1e1_steps90110:
                            - 91.43%

        - 220802123001_bs128_sgd_lr1e1_steps90110_c4256d:
                  - 94.33%
        - 220802123001_bs128_sgd_lr1e1_steps90110:
                  - 95.04%
        - 220802123001_bs128_sgd_lr1e1_steps90110_gcnfpn1_multit357357357shared:
                  - 95.94%

sgn_v13:
        - the G attention matrix is further augmented with a T attention matrix.
        - GT1 is matmul
        - GT2 is pointwise mul
        # WRONG ALPHA PLACEMENTS --------------
        - 220825143001_alpha005:
                  - 94.43%
        - 220825143001_alpha001:
                  - 94.43%
        - 220825143001_alpha05:
                  - 94.43%
        - 220825143001_alpha01:
                  - 94.43%
        - 220825153001_gt2_alpha01:
                  - 93.66%
        - 220825153001_gt2_alpha05:
                  - 93.66%
        # -------------------------------------
        - 220824167001:
                  - 94.43%
        - 220824167001_tmode3:
                  - 94.64%
        - 220825170001_alpha05:
                  - 94.37%
        - 220825170001_alpha01:
                  - 92.36%

        - 220825153001_gt2:
                  - 93.66%
        - 220825170001_gt2_alpha05:
                  - 94.04%
        - 220825170001_gt2_alpha01:
                  - 94.67%
        - 220825170001_gt2_alpha005:
                  - 94.60%
        - 220825170001_gt2_alpha001:
                  - 93.80%

        - 220826113001_sigmoid:
                  - 89.77%
        - 220826113001_alpha01_sigmoid:
                  - 93.60%
        - 220826113001_alpha05_sigmoid:
                  - 90.12%
        - 220826113001_gt2_sigmoid:
                  - 94.69%
        - 220826113001_gt2_alpha05_sigmoid:
                  - 94.67%
        - 220826113001_gt2_alpha01_sigmoid:
                  - 92.74%

        - 220826153001_tmode0:
                  - 92.52%
        - 220826153001_varalpha:
                  - tmode 0
                  - 92.61%
        - 220829113001_varalpha:
                  - 94.54%
        - 220826153001_gt2_tmode0:
                  - 88.87%
        - 220826153001_gt2_varalpha:
                  - tmode 0
                  - 91.32%
        - 220829113001_gt2_varalpha:
                  - 94.39%
        - 220829113001_gt2_varalpha_sigmoid:
                  - 94.72%

        - 220829113001_gt2_varalpha_sigmoid_lr001:
                  - 86.34%

        - 220830170001_gt2_fpn10_sigmoid:
                  - varalpha
                  - lr = 0.01
                  - 88.17%
        - 220831100001_gt2_fpn10_sigmoid:
                  - varalpha
                  - 94.45%
        - 220831170001_gt2_fpn10_sigmoid_allvaralpha_multit333:
                  - 94.34%
        - 220831170001_gt2_fpn10_sigmoid_allvaralpha_multit333shared:
                  - 94.50%
        - 220831170001_gt2_fpn10_sigmoid_allvaralpha_multit357357357shared:
                  - 94.93%
        - 220831170001_gt2_fpn10_sigmoid_allvaralpha:
                  - 94.50%
        - 220831170001_gt2_fpn10_sigmoid_labelsmooth02:
                  - 93.89%

        - 220901230001_gt2_varalpha_sigmoid_fl05:
                  - 77.14%
        - 220901230001_gt2_varalpha_sigmoid_fl2:
                  - 94.74%
        - 220901230001_gt2_varalpha_sigmoid_fl1:
                  - 94.78%

        - FAULTY, no gcn = 10:
                  - 220831230001_gt3_gcnfpn10_sigmoid_allvaralpha:
                            - 94.92%
                  - 220831230001_gt3_sigmoid_allvaralpha:
                            - 94.56%
                  - 220831230001_gt3_gcnfpn10_sigmoid_allvaralpha_fl05:
                            - 94.41%
                  - 220831230001_gt3_gcnfpn10_sigmoid_allvaralpha_fl1:
                            - 95.00%
                  - 220831230001_gt3_gcnfpn10_sigmoid_allvaralpha_fl2:
                            - 94.56%
                  - 220831230001_gt3_gcnfpn10_sigmoid_allvaralpha_fl3:
                            - 94.41%

        - 220901230001_gt3_fpn10_sigmoid_allvaralpha_fl05:
                  - 55.51%
        - 220901230001_gt3_fpn10_sigmoid_allvaralpha_fl1:
                  - 94.50%
        - 220901230001_gt3_fpn10_sigmoid_allvaralpha_fl2:
                  - 94.39%
        - 220901230001_gt3_sigmoid_allvaralpha:
                  - 94.56%
        - 220901230001_gt3_sigmoid_allvaralpha_fl05:
                  - 78.24%
        - 220901230001_gt3_sigmoid_allvaralpha_fl1:
                  - 95.03%
        - 220901230001_gt3_sigmoid_allvaralpha_fl2:
                  - 94.80%
        - 220901230001_gt3_idx3_sigmoid_allvaralpha_fl1:
                  - 94.86%
        - 220901230001_gt3_idx4_sigmoid_allvaralpha_fl1:
                  - 94.61%
        - 220901230001_gt3_idx5_sigmoid_allvaralpha_fl1:
                  - 94.60%

        - 220901230001_gt3_varalpha_sigmoid_multit333_fl1:
                  - 95.01%

        # gt4 uses 2 layer tcn for attention T (like gt3)
        # and multiplies with input x in gcn
        - 220915120001_gt4_varalpha_sigmoid_multit357:
                  - 95.18%
        - 220915120001_gt4_varalpha_sigmoid:
                  - 94.55%
        - 220902150001_gt4_varalpha_sigmoid:
                  - t 357
                  - 95.18%
        - 220902150001_gt4_varalpha_sigmoid_flgamma05:
                  - t 357
                  - 76.86%
        - 220902150001_gt4_varalpha_sigmoid_flgamma1:
                  - t 357
                  - 95.12%
        - 220902150001_gt4_varalpha_sigmoid_flgamma2:
                  - t 357
                  - 94.91%
        - 220905143001_gt4_varalpha:
                  - softmax
                  - 77.45%
        - 220906143001_gt4_varalphazero:
                  - 91.86%

        - 220906163001_gt5_varalpha:
                  - uses G as input for prediction also.
                  - 94.84%
        - 220906163001_gt5_varalpha_outputmerge2:
                  - uses G as input for prediction also.
                  - 94.80%
        - 220906163001_gt5_varalpha_allactnorm:
                  - 94.65%
        - 220906163001_gt5_varalpha_nogttemsem:
                  - 94.61%
        - 220906163001_gt5_varalpha_allactnorm_30fr:
                  - 94.51%
        - 220908150001_gt6_varalpha_multit357:
                  - 95.00%
        - 220908150001_gt6_varalpha:
                  - 94.56%
        - 220906163001_gt5_varalpha_allactnorm_64fr:
                  - 94.21%

        # feature similarity FAULTY, without 1- x
        - 220908210001_gt6_varalpha_fsim1alpha001:
                  - 94.69%
        - 220908210001_gt6_varalpha_fsim1alpha01:
                  - 94.48%
        - 220908210001_gt6_varalpha_fsim1alpha1:
                  - 94.17%
        - 220908210001_gt6_varalpha_fsim1alpha10:
                  - 93.24%
        - 220908210001_gt6_varalpha_fsim2alpha001:
                  - 94.58%
        - 220908210001_gt6_varalpha_fsim2alpha01:
                  - 94.30%
        - 220908210001_gt6_varalpha_fsim2alpha1:
                  - 94.51%
        - 220908210001_gt6_varalpha_fsim2alpha10:
                  - 94.45%

        # feature similarity
        - 220909150001_gt6_varalpha_fsim2alpha001:
                  - 94.51%
        - 220909150001_gt6_varalpha_fsim2alpha01:
                  - 94.73%
        - 220909150001_gt6_varalpha_fsim2alpha1:
                  - 94.64%
        - 220909150001_gt6_varalpha_fsim2alpha10:
                  - 94.45%
        - 220909150001_gt6_varalpha_fsim1alpha001:
                  - 94.58%
        - 220909150001_gt6_varalpha_fsim1alpha01:
                  - 94.50%
        - 220909150001_gt6_varalpha_fsim1alpha1:
                  - 94.17%
        - 220909150001_gt6_varalpha_fsim1alpha10:
                  - 92.94%

        - MLP temporal has residual (not intended):
                  - 220920150001_tmode4_k9:
                            - 94.85%
                  - 220920150001_tmode4_k7:
                            - 94.86%
                  - 220920150001_tmode4_k5:
                            - 94.99%
                  - 220920140001_tmode4:
                            - 94.87%
                  - 220920140001_gt4_varalpha_tmode4:
                            - 80.37%

        - 220922140001_gt4_varalpha_sigmoid_tmode4_k3:
                  - 95.03%
        - 220922140001_gt4_varalpha_sigmoid_tmode4_k5:
                  - 95.12%
        - 220922140001_gt4_varalpha_sigmoid_tmode4_k7:
                  - 94.91%
        - 220922140001_gt4_varalpha_sigmoid_tmode4_k9:
                  - 94.94%

sgn_v14:
        # GT=1 #################################################################
        - 220923140001_tmode5_9:
                  - 92.91%
        - 220923140001_tmode5_7:
                  - 93.28%
        - 220923140001_tmode5_5:
                  - 93.75%
        - 220923140001_tmode5_3:
                  - 94.04%
        - 220923140001_tmode5_35:
                  - 94.08%
        - 220923140001_tmode5_357:
                  - 94.44%
        - 220923140001_tmode5_3579:
                  - 94.40%
        - 220923140001_tmode5_1357_multit111:
                  - 94.50%
        - 220923140001_tmode5_1357:
                  - 94.78%
        - 220923140001_tmode1_aspp1357:
                  - 94.05%
        - 220928170001_tmode3_1layer:
                  - 94.69%
        - 220928170001_tmode3_1layer_3heads:
                  - 94.78%
        - 220928170001_tmode3:
                  - 94.80%
        - 220928170001_tmode3_3heads:
                  - 94.77%
        - 220928170001_tmode3_3layers:
                  - 94.45%
        - 220928170001_tmode3_4layers:
                  - 94.54%
        - 220928170001_tmode3_gt4_var_sigmoid:
                  - 94.60%
        - 220928170001_tmode3_1layer_sgcnattn1:
                  - 93.74%
        - 220928170001_tmode3_1layer_6heads:
                  - 94.51%
        - 220928170001_tmode3_1layer_9heads:
                  - 94.67%
        - 220928170001_tmode3_1layer_3heads_1024dhead:
                  - 94.42%
        - 220928170001_tmode3_2layer_6heads:
                  - 94.71%
        - 220928170001_tmode3_2layer_9heads:
                  - 94.52%
        - 221006150001_tmode3_cosposenc:
                  - 94.55%
        - 221006150001_tmode3_absposenc:
                  - 94.82%
        - 221006150001_tmode3_drop2d01:
                  - 94.55%

        - 221006150001_tmode3_9heads_cosposenc:
                  - 94.08%
        - 221006150001_tmode3_6heads_cosposenc:
                  - 94.21%
        - 221006150001_tmode3_3heads_cosposenc:
                  - 94.40%

        - 221006150001_tmode3_3heads_absposenc:
                  - 94.44%
        - 221006150001_tmode3_6heads_absposenc:
                  - 94.51%
        - 221006150001_tmode3_9heads_absposenc:
                  - 94.17%

        - 221010140001_tmode3_3heads_256ffn_nosemfr:
                  - 93.74%
        - 221010140001_tmode3_3heads_256ffn:
                  - 94.51%
        - 221010140001_tmode3_256ffn:
                  - 94.53%
        - 221010140001_tmode3_3heads_256ffn_absposenc:
                  - 94.53%
        # ######################################################################

        - 221010140001_tmode3_3heads_256ffn_gt0:
                  - 94.49%
        - 221010140001_tmode3_3heads_256ffn_noshartedg_gt0:
                  - 94.10%
        - 221010140001_tmode3_3heads_256ffn_noshartedg:
                  - visual inspection shows that the gcn learned to attend only at => head + shoulder + elbows, hips + toes, fingers
                  - 94.71%
        - 221011110001_tmode3_1layer_3heads_256ffn:
                  - 94.60%
        - 221011110001_gcn4layers_tmode3_1layer_3heads_256ffn:
                  - 94.46%
        - 221011110001_gcn5layers_tmode3_1layer_3heads_256ffn:
                  - 93.87%

        - 221011110001_tmode3_1layer_3heads_256ffn_noshartedg:
                  - visual inspection shows that the gcn learned to attend only at => spine, arms, stomach
                  - 94.37%
        - 221011110001_gcn4layers_tmode3_1layer_3heads_256ffn_noshartedg:
                  - visual inspection shows that the gcn learned to attend only at => head, hips + legs (faint-ish), upper torso, right hand only
                  - 94.45%
        - 221011110001_gcn5layers_tmode3_1layer_3heads_256ffn_noshartedg:
                  - visual inspection shows that the gcn learned to attend only at => hips, legs, right hand/finger only, one side of hip when static, legs when jumping/large movement, head
                  - 94.32%
        - 221011110001_gcnffn1_tmode3_1layer_3heads_256ffn_noshartedg:
                  - visual inspection shows that the gcn learned to attend only at => 0 + (faint) legs, head, legs
                  - 94.04%

        # REFERENCE RESULT: 3gcn-gcnffn + 1 temporal mha with 3 heads
        - 221011110001_gt0_gcnffn1_tmode3_1layer_3heads_256ffn_noshartedg:
                  - 94.73%

        # with and without gcn ffn
        - 221017160001_gt0_1gcn_tmode3_1layer_9heads_256ffn_noshartedg_drop01:
                  - 93.05%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_9heads_256ffn_noshartedg_drop01:
                  - 94.72%

        # Different number of heads for temporal branch + gt0
        - 221011110001_gt0_1gcn_gcnffn1_tmode3_1layer_3heads_256ffn_noshartedg:
                  - 94.60%
        - 221011110001_gt0_1gcn_gcnffn1_tmode3_1layer_6heads_256ffn_noshartedg:
                  - 94.48%
        - 221011110001_gt0_1gcn_gcnffn1_tmode3_1layer_9heads_256ffn_noshartedg:
                  - 94.73%
        - 221011110001_gt0_1gcn_gcnffn1_tmode3_1layer_12heads_256ffn_noshartedg:
                  - 94.67%
        - 221011110001_gt0_1gcn_gcnffn1_tmode3_1layer_15heads_256ffn_noshartedg:
                  - 94.23%

        # Different number of heads for temporal branch + gt1
        - 221011110001_1gcn_gcnffn1_tmode3_1layer_3heads_256ffn_noshartedg:
                  - 94.40%
        - 221011110001_1gcn_gcnffn1_tmode3_1layer_9heads_256ffn_noshartedg:
                  - 94.02%
        - 221011110001_1gcn_gcnffn1_tmode3_1layer_12heads_256ffn_noshartedg:
                  - 94.27%
        - 221011110001_1gcn_gcnffn1_tmode3_1layer_15heads_256ffn_noshartedg:
                  - 94.23%

        # Different num of heads and head dims for temporal MHA
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_8dim_256ffn_noshartedg_drop01:
                  - 94.58%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_16heads_8dim_256ffn_noshartedg_drop01:
                  - 94.85%                  
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_rerun:
                  - 94.90%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_16heads_16dim_256ffn_noshartedg_drop01:
                  - 94.91%                  
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_32dim_256ffn_noshartedg_drop01:
                  - 94.87%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_16heads_32dim_256ffn_noshartedg_drop01:
                  - 94.76%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_64dim_256ffn_noshartedg_drop01:
                  - 94.77%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_16heads_64dim_256ffn_noshartedg_drop01:
                  - 94.80%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_128dim_256ffn_noshartedg_drop01:
                  - 94.75%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_16heads_128dim_256ffn_noshartedg_drop01:
                  - 94.70%

        # different ffn dims
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_512ffn_noshartedg_drop01:
                  - 95.01%
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_1024ffn_noshartedg_drop01:
                  - 94.99%
        - 221020170001_gt0_1gcn_gcnffn2_tmode3_1layer_8heads_16dim_512ffn_noshartedg_drop01:
                  - 94.95%
        - 221020170001_gt0_1gcn_gcnffn4_tmode3_1layer_8heads_16dim_1024ffn_noshartedg_drop01:
                  - 94.72%

        # Lower dim for gcn proj + output FC dim
        - 221017160001_gt0_1gcn_128dim_gcnffn1_tmode3_1layer_8heads_16dim_128ffn_256out_noshartedg_drop01:
                  - gcn 128, proj 256
                  - 93.40%
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_256out_noshartedg_drop01:
                  - 94.20%

        # higher ffn dim, 2 layers of temporal MHA
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_256ffn_noshartedg_drop01:
                  - SMP4
                  - 94.98%
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_512ffn_512ffn_noshartedg_drop01:
                  - SMP4
                  - 94.92%

        # higer fc dim. folder title is wrong
        - 221019120001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_1024ffn_noshartedg_drop01:
                  - 95.02%
        - 221019120001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_2048ffn_noshartedg_drop01:
                  - 94.80%
        - 221019120001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_4096ffn_noshartedg_drop01:
                  - 94.31%

        # Added semjointsmp (extra joint emb before smp)
        - 256ffn_noshartedg_smp3_semjointsmp1:
                  - 221013160001_gt0_1gcn_gcnffn1_tmode3_1layer_9heads_
                  - semjointsmp1
                  - 94.63%
        - 256ffn_noshartedg_smp4_semjointsmp1:
                  - 221013160001_gt0_1gcn_gcnffn1_tmode3_1layer_9heads_
                  - semjointsmp1
                  - 94.44%
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_smp3:
                  - 94.90%
        - 221020170001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_smp4:
                  - 94.93%

        # Using agcn data
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_9heads_256ffn_noshartedg_drop01_aagcndata:
                  - 92.77%
        # using agcn + gcn data
        - 221017160001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_sgnaagcndata:
                  - 95.03%

        - 221031110001_gt0_1gcn_gcnffn05_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.63%
        - 221031110001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - same as previous ... rerun
                  - 94.90%
        - 221031110001_gt0_1gcn_gcnffn101_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.54%
        - 221031110001_gt0_1gcn_gcnffn201_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.68%
        - 221031110001_gt0_1gcn_gcnffn301_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.46%
        - 221031110001_gt0_1gcn_gcnffn301_1act_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - FAULTY
                  - 94.46%
        - 221031110001_gt0_1gcn_gcnffn301_1act_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_rerun:
                  - 94.59%

        - 221031110001_gt0_1gcn_gcnffn4_tmode3_1layer_8heads_16dim_1024ffn_noshartedg_drop01:
                  - 94.78%
        - 221031110001_gt0_1gcn_gcnffn4_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.78%
        - 221031110001_gt0_1gcn_gcnffn2_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - 94.67%

        - 221031110001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01:
                  - copy from above
                  - same as previous ... rerun
                  - 94.90%
        - 221104130001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_sgd_lr1e1_steps90110:
                  - 94.47%
        - 221104130001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_bs128_sgd_lr1e1_steps90110:
                  - 95.02%
        - 221104130001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_bs256_sgd_lr1e1_steps90110:
                  - 94.91%
        - 221104130001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_bs512_sgd_lr1e1_steps90110:
                  - 94.05%
        - 221104130001_gt0_1gcn_gcnffn1_tmode3_1layer_8heads_16dim_256ffn_noshartedg_drop01_bs1024_sgd_lr1e1_steps90110:
                  - 93.41%

sgn_v15:
        - only transformers
        - 2210121800:
                  - 93.68%
        - 2210121800_9_1heads:
                  - 93.65%

        - 2210121800_mhadimmult1:
                  - 93.90%
        - 2210121800_mhadimmult1_3heads:
                  - 94.29%
        - 2210121800_mhadimmult1_6heads:
                  - 94.43%
        - 2210121800_mhadimmult1_9heads:
                  - 94.43%
        - 2210121800_mhadimmult1_9_1heads:
                  - 94.62%
        - 2210121800_mhadimmult1_9_3heads:
                  - 94.37%
        - 2210131700_mhadimmult1_3_1heads:
                  - 93.85%

        - 2210141000_mhadimmult1_3_1heads:
                  - 93.85%
        - 2210141000_mhadimmult1_6_1heads:
                  - 94.28%
        - 2210141000_mhadimmult1_9_1heads:
                  - 94.62%
        - 2210141000_mhadimmult1_12_1heads:
                  - 94.14%
        - 2210141000_mhadimmult1_12_12heads:
                  - 94.18%

        - 2210121800_2layers:
                  - 92.62%
        - 2210121800_mhadimmult1_2layers:
                  - 94.16%
        - 2210121800_mhadimmult1_3heads_2layers:
                  - 94.35%

        - 2210131700_mhadimmult1_drop02:
                  - 93.43%

        - 2210181800_mhadimmult1_1_8heads_16dim:
                  - 94.23%

        - 2210181800_mhadimmult1_1_8heads_16dim_ln:
                  - 93.12%

        - 221031120001_1head_256hdim_256out_nov_res_8head_16hdim_bn_rerun:
                  - 93.50%
        - 221031120001_1head_256hdim_256out_8head_16hdim_bn_rerun:
                  - 93.41%

        - 221031120001_1head_256hdim_256out_8head_16hdim_ln:
                  - 92.70%

sgn-code:
        - original sgn repo code
        - 220324170001:
                  - 94.5%
        - 220324170001_rerun:
                  - 94.2%
        - 220324170001_rerun2:
                  - 94.2%
        - 220324170001_rerun3:
                  - 94.2%
        - 220324170001_rerun4:
                  - 94.2%
        - 220324170001_rerun5:
                  - 94.2%
        - 220324170001_rerun6_16workers:
                  - 94.5%
        - 220324170001_rerun7_32workers:
                  - 94.0%
