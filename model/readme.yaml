aagcn:
  - original model:
      - 94.7%
      - overfit, train acc ~1
  - original model, new datagen:
      - 94.4%
      - overfit, train acc ~1
  - original model, new datagen, no attention:
      - 94.1%
      - overfit, train acc ~1
      - not much difference compared to the one with attention

aagcn_v2:
  - A does not depend on predefined A matrix. > no noticable difference.
  - Using 1 subset:
      - no noticable difference. 94.3%
      - overfit, train acc ~1
  - Using 3 subset:
      - no noticable difference. 94.1%
      - overfit, train acc ~1
  - Using 5 subset:
      - no noticable difference. 94.3%
      - overfit, train acc ~1

aagcn_v3:
  - Added additional projection in the GCN.
  - no noticable changes. 94.6%
  - overfit, train acc ~1

aagcn_v4:
  - Merged TCN into GCN.
  - Create different sets of subsets.
  - Each set will have different kernel size in temporal dim. (1,3,6,9)
  - did not converge

aagcn_v5:
  - added se block after TCN, sct attention is removed:
      - slightly worse 93.72%
      - overfit, train acc ~1
  - without the tse block, sct attention is removed.

aagcn_v6:
  - removed TCN. Added tcn conv in AdaptiveGCN (conv_d)
  - slightly worst 93%
  - overfit, train acc ~1

aagcn_v7:
  - removed TCN, added TSE in the AdaptiveGCN.
  - worst, 92.1%

aagcn_v8:
  - creates multiple attentions instead of one in AdaptiveGCN
  - using 5 splits:
      - no noticable difference. 94.5%
      - overfit, train acc ~1
  - using 3 splits:
      - no noticable difference. 94.5%
      - overfit, train acc ~1

aagcn_v9:
  - uses lstm before classification.
  - GAP-TV, proj4:
    - 93.2%
    - overfit, faster than original model, train acc ~1
  - GAP-TV, proj4, bi:
    - 93.7%
    - overfit, faster than original model, train acc ~1
    - not much difference compared to lstm

aagcn_v10:
  - uses MHA before classification.
  - GAP-TV, 1h:
      - 92%
      - overfit, faster than original model, train acc ~1
  - GAP-TV, 2h:
      - 92.4%
      - overfit, faster than original model, train acc ~1
  - GAP-T, 1h:
      - 91.2%
      - overfit, slower than original model, train acc ~1
  - GAP-T, 2h:
      - 91%
      - overfit, slower than original model, train acc ~1
